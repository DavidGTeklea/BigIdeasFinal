{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnHDUptYTnED4ro/MD/Uwf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidGTeklea/BigIdeasFinal/blob/main/JustRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "bQtdGojlDRLe",
        "outputId": "745b7bf3-8ea0-4e5d-e0a6-9216e38a1cd7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0c570d42-a18a-4e84-beb8-906b6b47c805\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0c570d42-a18a-4e84-beb8-906b6b47c805\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Dataset URL: https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents\n",
            "License(s): CC-BY-NC-SA-4.0\n",
            "Downloading us-accidents.zip to /content/data/usacc\n",
            " 99% 649M/653M [00:10<00:00, 70.6MB/s]\n",
            "100% 653M/653M [00:10<00:00, 65.4MB/s]\n",
            "Archive:  /content/data/usacc/us-accidents.zip\n",
            "  inflating: /content/data/usacc/US_Accidents_March23.csv  \n",
            "Selected CSV_PATH: /content/data/usacc/US_Accidents_March23.csv\n",
            "USACC_PATH set for builder: /content/data/usacc/US_Accidents_March23.csv\n"
          ]
        }
      ],
      "source": [
        "# ---- 0) Set a data folder ----\n",
        "import os, glob\n",
        "DATA_DIR = \"/content/data/usacc\"       # choose any folder you like\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# ---- 1) Install Kaggle CLI ----\n",
        "!pip -q install -U kaggle\n",
        "\n",
        "# ---- 2) Upload kaggle.json (from your computer) ----\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # select kaggle.json\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# ---- 3) Download + unzip ----\n",
        "!kaggle datasets download -d sobhanmoosavi/us-accidents -p \"{DATA_DIR}\" -o\n",
        "!unzip -o \"{DATA_DIR}/us-accidents.zip\" -d \"{DATA_DIR}\"\n",
        "\n",
        "# ---- 4) Pick the main CSV (largest file) ----\n",
        "csvs = glob.glob(os.path.join(DATA_DIR, \"*.csv\"))\n",
        "CSV_PATH = max(csvs, key=os.path.getsize)\n",
        "print(\"Selected CSV_PATH:\", CSV_PATH)\n",
        "\n",
        "# ---- 5) Wire to your pipeline ----\n",
        "USACC_PATH = CSV_PATH          # or: RAW_CSV = CSV_PATH\n",
        "print(\"USACC_PATH set for builder:\", USACC_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- one-time install for Parquet ---\n",
        "!pip -q install pyarrow\n",
        "\n",
        "import numpy as np, pandas as pd, pyarrow as pa, pyarrow.parquet as pq\n",
        "from pathlib import Path\n",
        "\n",
        "USACC_PATH = CSV_PATH  # your existing variable (raw CSV)\n",
        "\n",
        "RAW_CSV = globals().get(\"USACC_PATH\", None)\n",
        "if RAW_CSV is None:\n",
        "    raise RuntimeError(\"Set USACC_PATH (or RAW_CSV) to the raw CSV path first (kagglehub/CLI).\")\n",
        "\n",
        "OUT_PARQUET = \"/content/US_Accidents_duration_fast.parquet\"\n",
        "SUBSET_FRAC = 0.02\n",
        "CHUNK = 200_000\n",
        "\n",
        "# We do NOT read End_* coords or Distance (leaky); we read End_Time only to make the label\n",
        "USECOLS = [\n",
        "    \"ID\", \"Source\",\n",
        "    \"Start_Time\", \"End_Time\",\n",
        "    \"Start_Lat\", \"Start_Lng\",\n",
        "    \"Street\", \"City\", \"County\", \"State\", \"Zipcode\", \"Country\", \"Timezone\",\n",
        "    \"Weather_Timestamp\",\n",
        "    \"Weather_Condition\", \"Temperature(F)\", \"Humidity(%)\", \"Visibility(mi)\",\n",
        "    \"Wind_Speed(mph)\", \"Wind_Chill(F)\", \"Pressure(in)\", \"Precipitation(in)\",\n",
        "    \"Sunrise_Sunset\", \"Amenity\", \"Bump\", \"Crossing\", \"Give_Way\", \"Junction\",\n",
        "    \"No_Exit\", \"Railway\", \"Roundabout\", \"Station\", \"Stop\",\n",
        "    \"Traffic_Calming\", \"Traffic_Signal\", \"Turning_Loop\",\n",
        "    # NOTE: not reading Description, End_Lat, End_Lng, Distance(mi)\n",
        "]\n",
        "\n",
        "WEATHER_COLS = [\n",
        "    \"Temperature(F)\",\"Wind_Chill(F)\",\"Humidity(%)\",\"Pressure(in)\",\n",
        "    \"Visibility(mi)\",\"Wind_Speed(mph)\",\"Precipitation(in)\",\"Weather_Condition\"\n",
        "]\n",
        "\n",
        "def build_fast_sample_duration(csv_path: str, out_parquet: str, frac: float = 0.10,\n",
        "                               seed: int = 2025, chunksize: int = 200_000):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    writer, total_in, total_kept = None, 0, 0\n",
        "\n",
        "    for chunk in pd.read_csv(csv_path, usecols=USECOLS, chunksize=chunksize, low_memory=True):\n",
        "        total_in += len(chunk)\n",
        "        chunk = chunk.sort_values(\"Start_Time\")  # Sort each chunk for timeseriessplit later\n",
        "\n",
        "        # --- times\n",
        "        st = pd.to_datetime(chunk[\"Start_Time\"], errors=\"coerce\", utc=True)\n",
        "        et = pd.to_datetime(chunk[\"End_Time\"],   errors=\"coerce\", utc=True)\n",
        "        wt = pd.to_datetime(chunk.get(\"Weather_Timestamp\", pd.Series(pd.NaT, index=chunk.index)),\n",
        "                            errors=\"coerce\", utc=True)\n",
        "\n",
        "        # --- duration (min); require positive & finite\n",
        "        dur = (et - st).dt.total_seconds() / 60.0\n",
        "        m = np.isfinite(dur) & (dur > 0)\n",
        "        if not m.any():\n",
        "            continue\n",
        "        chunk = chunk.loc[m].copy()\n",
        "        dur = dur[m]\n",
        "        st, et, wt = st[m], et[m], wt[m]\n",
        "\n",
        "        # --- clamp tails per chunk to stabilize training\n",
        "        lo, hi = np.nanpercentile(dur, [1, 99])\n",
        "        chunk[\"duration_min\"] = np.clip(dur, lo, hi)\n",
        "\n",
        "        # --- weather leakage guard\n",
        "        mask_ok = wt.notna() & st.notna() & (wt <= st)\n",
        "        if WEATHER_COLS:\n",
        "            for c in WEATHER_COLS:\n",
        "                if c in chunk.columns:\n",
        "                    chunk.loc[~mask_ok, c] = np.nan\n",
        "        lag = (st - wt).dt.total_seconds() / 60.0\n",
        "        chunk[\"weather_lag_min\"] = np.where(mask_ok, lag, np.nan)\n",
        "\n",
        "        # --- start-time engineered parts for temporal splitting / features\n",
        "        chunk[\"start_hour\"]  = st.dt.hour.astype(\"Int64\")\n",
        "        chunk[\"start_wday\"]  = st.dt.dayofweek.astype(\"Int64\")\n",
        "        chunk[\"start_month\"] = st.dt.month.astype(\"Int64\")\n",
        "        chunk[\"start_year\"]  = st.dt.year.astype(\"Int64\")\n",
        "\n",
        "        # --- uniform downsample\n",
        "        keep = rng.rand(len(chunk)) < frac\n",
        "        if not keep.any():\n",
        "            continue\n",
        "        sm = chunk.loc[keep].copy()\n",
        "\n",
        "        # --- drop definite leakers / non-predictive keys before writing\n",
        "        sm.drop(columns=[c for c in [\n",
        "            #\"Start_Time\",          # keep derived parts instead\n",
        "            \"End_Time\",            # used only for label\n",
        "            \"Weather_Timestamp\",   # used only for guard\n",
        "            \"ID\"                   # row key; can spuriously help\n",
        "        ] if c in sm.columns], inplace=True)\n",
        "\n",
        "        # write → Parquet\n",
        "        tbl = pa.Table.from_pandas(sm, preserve_index=False)\n",
        "        if writer is None:\n",
        "            writer = pq.ParquetWriter(out_parquet, tbl.schema)\n",
        "        writer.write_table(tbl)\n",
        "        total_kept += len(sm)\n",
        "\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "    print(f\"Stream-sampled {total_kept:,} rows out of ~{total_in:,} into {out_parquet}\")\n",
        "\n",
        "build_fast_sample_duration(RAW_CSV, OUT_PARQUET, frac=SUBSET_FRAC, seed=2025, chunksize=CHUNK)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smd4el7DDY_V",
        "outputId": "341298d2-1325-4e88-c104-d6982b6f8cf0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stream-sampled 134,541 rows out of ~7,728,394 into /content/US_Accidents_duration_fast.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a real module on disk so workers can import it\n",
        "from textwrap import dedent\n",
        "with open(\"cat_tokens.py\", \"w\") as f:\n",
        "    f.write(dedent(\"\"\"\n",
        "    from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "    class CatToTokens(BaseEstimator, TransformerMixin):\n",
        "        \\\"\\\"\\\"Turn categorical frame/array into list-of-tokens per row.\n",
        "        Tokens are 'col=value' to reduce collisions for FeatureHasher.\\\"\\\"\\\"\n",
        "        def __init__(self, prefix=True):\n",
        "            self.prefix = prefix\n",
        "            self.columns_ = None\n",
        "\n",
        "        def fit(self, X, y=None):\n",
        "            if hasattr(X, \"columns\"):\n",
        "                self.columns_ = [str(c) for c in X.columns]\n",
        "            else:\n",
        "                n_cols = X.shape[1] if hasattr(X, \"shape\") else len(X[0])\n",
        "                self.columns_ = [f\"c{i}\" for i in range(int(n_cols))]\n",
        "            return self\n",
        "\n",
        "        def transform(self, X):\n",
        "            if hasattr(X, \"to_numpy\"):\n",
        "                arr = X.astype(\"U\").to_numpy()\n",
        "            else:\n",
        "                arr = X.astype(\"U\")\n",
        "            cols = self.columns_\n",
        "            if self.prefix:\n",
        "                return [[f\"{c}={v}\" for c, v in zip(cols, row)] for row in arr]\n",
        "            else:\n",
        "                return [list(row) for row in arr]\n",
        "    \"\"\"))\n"
      ],
      "metadata": {
        "id": "uCzWSlzUD1C9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================== Regression prep (Duration + Hashing + MaxAbs + Caching) ===================\n",
        "import os, glob, time, json, pathlib\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from joblib import Memory\n",
        "import importlib, pyarrow.parquet as pq\n",
        "\n",
        "# --------- Config ----------\n",
        "SEED_SPLIT   = 2025\n",
        "NJOBS        = -1                     # use all cores\n",
        "DEVELOPMENT_MODE = True               # False for full run\n",
        "DEV_SAMPLE_FRAC  = 0.10               # 10% during dev\n",
        "BASE_OUT = Path(\"results/usaccidents_regression\"); BASE_OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# This must point to the DURATION parquet produced by build_fast_sample_duration(...)\n",
        "USACC_PARQUET_FAST = OUT_PARQUET\n",
        "\n",
        "# --------- Fresh cache dir (so nothing stale is reused) ----------\n",
        "import shutil\n",
        "shutil.rmtree(\"cache_usacc_reg_hash_tok_v3\", ignore_errors=True)\n",
        "CACHE_DIR = Path(\"cache_usacc_reg_hash_tok_v3\"); CACHE_DIR.mkdir(exist_ok=True)\n",
        "memory = Memory(location=str(CACHE_DIR), verbose=0)\n",
        "\n",
        "# --------- Import the picklable transformer from the module you wrote in Step 1 ----------\n",
        "importlib.invalidate_caches()\n",
        "from cat_tokens import CatToTokens\n",
        "\n",
        "from sklearn.preprocessing import MaxAbsScaler, FunctionTransformer\n",
        "from sklearn.feature_extraction import FeatureHasher\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --------- Read Parquet (schema-only, then needed columns) ----------\n",
        "print(\"Parquet path:\", USACC_PARQUET_FAST)\n",
        "pf = pq.ParquetFile(USACC_PARQUET_FAST)\n",
        "df_columns = pf.schema.names\n",
        "n_rows = pf.metadata.num_rows\n",
        "print(f\"Parquet rows: {n_rows:,} | columns: {len(df_columns)}\")\n",
        "\n",
        "LEAKY  = [\"End_Time\",\"End_Lat\",\"End_Lng\",\"Distance(mi)\",\"Weather_Timestamp\",\"Description\",\"ID\",\"Airport_Code\"]\n",
        "TARGET = \"duration_min\"\n",
        "if TARGET not in df_columns:\n",
        "    raise RuntimeError(\n",
        "        f\"'duration_min' not found in {USACC_PARQUET_FAST}. \"\n",
        "        \"Point to the duration parquet (not onset) or rebuild the duration builder.\"\n",
        "    )\n",
        "\n",
        "needed_columns = [c for c in df_columns if c not in LEAKY]\n",
        "print(f\"Reading {len(needed_columns)} columns (skipping {len(LEAKY)} leaky cols)…\")\n",
        "df = pd.read_parquet(USACC_PARQUET_FAST, columns=needed_columns)\n",
        "\n",
        "# --------- Optional dev sampling ----------\n",
        "if DEVELOPMENT_MODE:\n",
        "    df = df.sample(frac=DEV_SAMPLE_FRAC, random_state=SEED_SPLIT)\n",
        "    print(f\"DEV mode: {len(df):,} rows\");\n",
        "else:\n",
        "    print(f\"PROD mode: {len(df):,} rows\")\n",
        "\n",
        "# --------- y / X and temporal split ----------\n",
        "y = df[TARGET].astype(float).values\n",
        "X = df.drop(columns=[TARGET]).copy()\n",
        "\n",
        "req = {\"start_year\",\"start_month\",\"start_wday\",\"start_hour\"}\n",
        "if not req.issubset(X.columns):\n",
        "    raise RuntimeError(\"Missing start_* parts. Rebuild with the duration builder cell.\")\n",
        "\n",
        "\n",
        "# order_key = (X[\"start_year\"].fillna(-1).astype(int)*10_000\n",
        "#              + X[\"start_month\"].fillna(-1).astype(int)*100\n",
        "#              + X[\"start_wday\"].fillna(-1).astype(int)*10\n",
        "#              + X[\"start_hour\"].fillna(-1).astype(int))\n",
        "# order = np.argsort(order_key.values)\n",
        "# cut = int(len(order)*0.80)\n",
        "# tr_idx, te_idx = order[:cut], order[cut:]\n",
        "\n",
        "# REPLACE with:\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "tscv = TimeSeriesSplit(n_splits=2)  # Creates 2 folds like your 80/20 split\n",
        "splits = list(tscv.split(X))\n",
        "tr_idx, te_idx = splits[-1]\n",
        "\n",
        "X_train, X_test = X.iloc[tr_idx], X.iloc[te_idx]\n",
        "y_train, y_test = y[tr_idx], y[te_idx]\n",
        "print(f\"Train: {X_train.shape} | Test: {X_test.shape}\")\n",
        "\n",
        "# --------- Hashing + MaxAbs preprocessor (sparse) and dense variant for MLP ----------\n",
        "HASH_FEATURES = 1024  # bump to 4096 if RAM allows\n",
        "\n",
        "def make_hashing_preprocess_reg(X_train, memory=None, dense=False, n_features=HASH_FEATURES):\n",
        "    cat_cols = [c for c in X_train.columns if X_train[c].dtype == \"object\"]\n",
        "    num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
        "\n",
        "    num_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))], memory=memory)\n",
        "    cat_pipe = Pipeline([\n",
        "        (\"imp\",  SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
        "        (\"tok\",  CatToTokens(prefix=True)),  # <— module class (picklable)\n",
        "        (\"hash\", FeatureHasher(n_features=n_features, input_type=\"string\", dtype=np.float32)),\n",
        "    ], memory=memory)\n",
        "\n",
        "    preprocess = ColumnTransformer([\n",
        "        (\"num\", num_pipe, num_cols),\n",
        "        (\"cat\", cat_pipe, cat_cols),\n",
        "    ], remainder=\"drop\", sparse_threshold=1.0, n_jobs=NJOBS)\n",
        "\n",
        "    steps = [(\"prep\", preprocess), (\"scale\", MaxAbsScaler())]\n",
        "    if dense:\n",
        "        steps.append((\"to_dense\", FunctionTransformer(\n",
        "            lambda X: X.toarray() if hasattr(X, \"toarray\") else X, accept_sparse=True\n",
        "        )))\n",
        "    return Pipeline(steps, memory=memory)\n",
        "\n",
        "# ---- Evaluation helper (model-agnostic) ----\n",
        "import pathlib, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def evaluate_regression(y_true, y_pred, outdir, label=\"model\", dev_flag=False):\n",
        "    outdir = pathlib.Path(outdir); outdir.mkdir(parents=True, exist_ok=True)\n",
        "    mae   = float(mean_absolute_error(y_true, y_pred))\n",
        "    medae = float(median_absolute_error(y_true, y_pred))\n",
        "    mse   = float(mean_squared_error(y_true, y_pred)); rmse = float(np.sqrt(mse))\n",
        "    r2    = float(r2_score(y_true, y_pred))\n",
        "    mape  = float(np.mean(np.abs((y_true - y_pred) / np.clip(np.abs(y_true), 1e-6, None))) * 100.0)\n",
        "\n",
        "    res = pd.DataFrame({\"y_true\": y_true, \"y_pred\": y_pred, \"resid\": y_true - y_pred})\n",
        "    res.to_csv(outdir / f\"residuals_{label}.csv\", index=False)\n",
        "\n",
        "    plt.figure(); plt.scatter(res[\"y_pred\"], res[\"resid\"], s=6, alpha=0.5)\n",
        "    plt.axhline(0, color=\"k\", linestyle=\"--\"); plt.xlabel(\"Predicted (min)\"); plt.ylabel(\"Residual\")\n",
        "    plt.title(f\"Residuals vs Prediction — {label}{' (DEV)' if dev_flag else ''}\")\n",
        "    plt.tight_layout(); plt.savefig(outdir / f\"residuals_scatter_{label}.png\", dpi=140); plt.close()\n",
        "\n",
        "    plt.figure(); plt.hist(res[\"resid\"], bins=50)\n",
        "    plt.xlabel(\"Residual (min)\"); plt.ylabel(\"Count\")\n",
        "    plt.title(f\"Residual Histogram — {label}{' (DEV)' if dev_flag else ''}\")\n",
        "    plt.tight_layout(); plt.savefig(outdir / f\"residuals_hist_{label}.png\", dpi=140); plt.close()\n",
        "\n",
        "    return {\"mae\": mae, \"medae\": medae, \"rmse\": rmse, \"mape_pct\": mape, \"r2\": r2}\n",
        "\n",
        "\n",
        "shared_reg       = make_hashing_preprocess_reg(X_train, memory, dense=False, n_features=HASH_FEATURES)\n",
        "shared_reg_dense = make_hashing_preprocess_reg(X_train, memory, dense=True,  n_features=HASH_FEATURES)  # for MLP\n",
        "\n",
        "print(\"\\nReady objects:\")\n",
        "print(\" - X_train, y_train, X_test, y_test\")\n",
        "print(\" - shared_reg       (sparse; hashing + MaxAbs)\")\n",
        "print(\" - shared_reg_dense (dense; for MLP)\")\n",
        "print(\" - BASE_OUT, DEVELOPMENT_MODE, HASH_FEATURES, NJOBS, CACHE_DIR\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmYbG8WWD4fk",
        "outputId": "4844adad-8d49-4f8f-f9d2-2594ef87cc43"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parquet path: /content/US_Accidents_duration_fast.parquet\n",
            "Parquet rows: 134,541 | columns: 39\n",
            "Reading 39 columns (skipping 8 leaky cols)…\n",
            "DEV mode: 13,454 rows\n",
            "Train: (8970, 38) | Test: (4484, 38)\n",
            "\n",
            "Ready objects:\n",
            " - X_train, y_train, X_test, y_test\n",
            " - shared_reg       (sparse; hashing + MaxAbs)\n",
            " - shared_reg_dense (dense; for MLP)\n",
            " - BASE_OUT, DEVELOPMENT_MODE, HASH_FEATURES, NJOBS, CACHE_DIR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## MLP Regressor\n",
        "from sklearn.experimental import enable_halving_search_cv  # noqa: F401\n",
        "from sklearn.model_selection import HalvingGridSearchCV\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.base import clone\n",
        "import numpy as np\n",
        "\n",
        "# Dense preprocessor for MLP\n",
        "shared_reg_dense = make_hashing_preprocess_reg(X_train, memory, dense=True, n_features=HASH_FEATURES)\n",
        "\n",
        "base_mlp = MLPRegressor(\n",
        "    hidden_layer_sizes=(64, 32),\n",
        "    activation=\"relu\",\n",
        "    solver=\"sgd\",\n",
        "    learning_rate=\"adaptive\",\n",
        "    learning_rate_init=0.01,\n",
        "    momentum=0.0,\n",
        "    nesterovs_momentum=False,\n",
        "    alpha=1e-4,\n",
        "    batch_size=256,\n",
        "    early_stopping=False,   # IMPORTANT during halving\n",
        "    max_iter=32,            # will be increased by halving\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "wrapped = TransformedTargetRegressor(\n",
        "    regressor=Pipeline([(\"shared\", shared_reg_dense), (\"mlp\", base_mlp)]),\n",
        "    func=np.log1p, inverse_func=np.expm1\n",
        ")\n",
        "\n",
        "param_grid = {\n",
        "    \"regressor__mlp__hidden_layer_sizes\": [(64,)],\n",
        "    \"regressor__mlp__learning_rate_init\": [0.01, 0.005],\n",
        "    \"regressor__mlp__alpha\": [1e-3, 1e-2],\n",
        "    \"regressor__mlp__batch_size\": [256],\n",
        "}\n",
        "\n",
        "inner_cv = TimeSeriesSplit(n_splits=3)\n",
        "cv = inner_cv\n",
        "hgs = HalvingGridSearchCV(\n",
        "    estimator=wrapped,\n",
        "    param_grid=param_grid,\n",
        "    resource=\"regressor__mlp__max_iter\",\n",
        "    min_resources=64, max_resources=512, factor=2,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=cv, n_jobs=NJOBS, verbose=1, refit=True, error_score=\"raise\"\n",
        ")\n",
        "\n",
        "hgs.fit(X_train, y_train)\n",
        "print(\"Best params:\", hgs.best_params_, \"Best CV MAE:\", -hgs.best_score_)\n",
        "\n",
        "# Evaluate the refit-from-halving model\n",
        "pred_refit = hgs.predict(X_test)\n",
        "metrics_refit = evaluate_regression(y_test, pred_refit, BASE_OUT, label=\"mlp_halving_refit\")\n",
        "print(\"Refit metrics:\", metrics_refit)\n",
        "\n",
        "# Optional: final polish with early stopping ON\n",
        "final = clone(hgs.best_estimator_)\n",
        "final.set_params(\n",
        "    **{\"regressor__mlp__early_stopping\": True,\n",
        "       \"regressor__mlp__validation_fraction\": 0.1,\n",
        "       \"regressor__mlp__max_iter\": 1000}\n",
        ")\n",
        "final.fit(X_train, y_train)\n",
        "pred_final = final.predict(X_test)\n",
        "metrics_final = evaluate_regression(y_test, pred_final, BASE_OUT, label=\"mlp_halving_ES\")\n",
        "print(\"Final (ES) metrics:\", metrics_final)\n",
        "\n",
        "train_median = float(np.median(y_train))\n",
        "baseline = evaluate_regression(y_test, np.full_like(y_test, train_median), BASE_OUT,\n",
        "                               label=\"baseline_median\", dev_flag=DEVELOPMENT_MODE)\n",
        "print({\"train_median_min\": train_median, **baseline})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ogSbuHRD-Ud",
        "outputId": "83f39f5e-2fb2-4053-aa70-6f0bfede1ec5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_iterations: 3\n",
            "n_required_iterations: 3\n",
            "n_possible_iterations: 4\n",
            "min_resources_: 64\n",
            "max_resources_: 512\n",
            "aggressive_elimination: False\n",
            "factor: 2\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 4\n",
            "n_resources: 64\n",
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 2\n",
            "n_resources: 128\n",
            "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
            "----------\n",
            "iter: 2\n",
            "n_candidates: 1\n",
            "n_resources: 256\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params: {'regressor__mlp__alpha': 0.01, 'regressor__mlp__batch_size': 256, 'regressor__mlp__hidden_layer_sizes': (64,), 'regressor__mlp__learning_rate_init': 0.01, 'regressor__mlp__max_iter': 256} Best CV MAE: 64.27844505370088\n",
            "Refit metrics: {'mae': 60.78642139157643, 'medae': 24.67716911597712, 'rmse': 131.83372298934614, 'mape_pct': 64.02865468498165, 'r2': 0.07688712982677248}\n",
            "Final (ES) metrics: {'mae': 60.31459091372482, 'medae': 23.521524682205467, 'rmse': 131.98641801732037, 'mape_pct': 64.19897343725678, 'r2': 0.07474751983905725}\n",
            "{'train_median_min': 60.0, 'mae': 67.88908742194471, 'medae': 30.283333333333335, 'rmse': 144.614580133217, 'mape_pct': 61.482678500772394, 'r2': -0.1107745854854012}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DT Regressor\n",
        "from sklearn.experimental import enable_halving_search_cv  # noqa: F401\n",
        "from sklearn.model_selection import HalvingGridSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "\n",
        "# pipeline: hashing+MaxAbs (sparse) -> DecisionTree\n",
        "dt = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "pipe_dt = TransformedTargetRegressor(\n",
        "    regressor=Pipeline([(\"shared\", shared_reg), (\"dt\", dt)]),\n",
        "    func=np.log1p, inverse_func=np.expm1\n",
        ")\n",
        "\n",
        "# Keep the grid tiny but targeted (NO max_depth here; halving controls it)\n",
        "param_grid = {\n",
        "    \"regressor__dt__criterion\": [\"absolute_error\", \"poisson\"],  # robust losses for skewed durations\n",
        "    \"regressor__dt__min_samples_leaf\": [100, 300],\n",
        "    \"regressor__dt__ccp_alpha\": [0.0, 1e-4],      # light pruning\n",
        "}\n",
        "\n",
        "inner_cv = TimeSeriesSplit(n_splits=3)\n",
        "cv = inner_cv\n",
        "\n",
        "hgs_dt = HalvingGridSearchCV(\n",
        "    estimator=pipe_dt,\n",
        "    param_grid=param_grid,\n",
        "    resource=\"regressor__dt__max_leaf_nodes\",  # capacity budget\n",
        "    min_resources=64,\n",
        "    max_resources=512,\n",
        "    factor=2,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=cv,\n",
        "    n_jobs=NJOBS,\n",
        "    verbose=1,\n",
        "    refit=True,\n",
        ")\n",
        "\n",
        "hgs_dt.fit(X_train, y_train)\n",
        "print(\"DT best:\", hgs_dt.best_params_)\n",
        "\n",
        "# Evaluate\n",
        "pred_dt = hgs_dt.predict(X_test)\n",
        "metrics_dt = evaluate_regression(y_test, pred_dt, BASE_OUT, label=\"dt_halving\", dev_flag=DEVELOPMENT_MODE)\n",
        "print(metrics_dt)\n",
        "\n",
        "# Median baseline (unchanged)\n",
        "train_median = float(np.median(y_train))\n",
        "baseline_metrics = evaluate_regression(\n",
        "    y_test, np.full_like(y_test, train_median), BASE_OUT,\n",
        "    label=\"baseline_median\", dev_flag=DEVELOPMENT_MODE\n",
        ")\n",
        "print({\"train_median_min\": train_median, **baseline_metrics})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S43SQntnEfge",
        "outputId": "911d7421-2dcc-40ab-f338-a35b94a4e4ec"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_iterations: 4\n",
            "n_required_iterations: 4\n",
            "n_possible_iterations: 4\n",
            "min_resources_: 64\n",
            "max_resources_: 512\n",
            "aggressive_elimination: False\n",
            "factor: 2\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 8\n",
            "n_resources: 64\n",
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 4\n",
            "n_resources: 128\n",
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
            "----------\n",
            "iter: 2\n",
            "n_candidates: 2\n",
            "n_resources: 256\n",
            "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
            "----------\n",
            "iter: 3\n",
            "n_candidates: 1\n",
            "n_resources: 512\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "DT best: {'regressor__dt__ccp_alpha': 0.0, 'regressor__dt__criterion': 'absolute_error', 'regressor__dt__min_samples_leaf': 100, 'regressor__dt__max_leaf_nodes': 512}\n",
            "{'mae': 41.10987378718458, 'medae': 15.0, 'rmse': 116.09245103109221, 'mape_pct': 45.10542996030805, 'r2': 0.28417026709634874}\n",
            "{'train_median_min': 60.0, 'mae': 67.88908742194471, 'medae': 30.283333333333335, 'rmse': 144.614580133217, 'mape_pct': 61.482678500772394, 'r2': -0.1107745854854012}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Linear SVR\n",
        "from sklearn.experimental import enable_halving_search_cv  # noqa: F401\n",
        "from sklearn.model_selection import HalvingGridSearchCV\n",
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "import numpy as np\n",
        "\n",
        "inner_cv = TimeSeriesSplit(n_splits=3)\n",
        "cv = inner_cv\n",
        "\n",
        "lin_svr = LinearSVR(\n",
        "    loss=\"squared_epsilon_insensitive\",  # <-- FIX\n",
        "    dual=False,                          # keep primal solver\n",
        "    max_iter=64,                         # resource; halving will grow this\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "pipe_lin = TransformedTargetRegressor(\n",
        "    regressor=Pipeline([(\"shared\", shared_reg), (\"svm\", lin_svr)]),\n",
        "    func=np.log1p, inverse_func=np.expm1\n",
        ")\n",
        "\n",
        "grid_lin = {\n",
        "    \"regressor__svm__C\":       [1e-2, 1e-1, 1, 10],\n",
        "    \"regressor__svm__epsilon\": [0.05, 0.1, 0.2],\n",
        "}\n",
        "\n",
        "hgs_lin = HalvingGridSearchCV(\n",
        "    estimator=pipe_lin,\n",
        "    param_grid=grid_lin,\n",
        "    resource=\"regressor__svm__max_iter\",\n",
        "    min_resources=64, max_resources=512, factor=3,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=cv, n_jobs=NJOBS, verbose=1, refit=True\n",
        ")\n",
        "\n",
        "hgs_lin.fit(X_train, y_train)\n",
        "lin_pred = hgs_lin.predict(X_test)\n",
        "lin_metrics = evaluate_regression(y_test, lin_pred, BASE_OUT, label=\"svm_linear\", dev_flag=DEVELOPMENT_MODE)\n",
        "print({\"svm_linear_best\": hgs_lin.best_params_, **lin_metrics})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycmKfMEkEqha",
        "outputId": "d30fefee-ff1f-47bb-82c4-75c964c90e83"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_iterations: 2\n",
            "n_required_iterations: 3\n",
            "n_possible_iterations: 2\n",
            "min_resources_: 64\n",
            "max_resources_: 512\n",
            "aggressive_elimination: False\n",
            "factor: 3\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 12\n",
            "n_resources: 64\n",
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 4\n",
            "n_resources: 192\n",
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
            "{'svm_linear_best': {'regressor__svm__C': 0.01, 'regressor__svm__epsilon': 0.05, 'regressor__svm__max_iter': 192}, 'mae': 60.30738537453757, 'medae': 24.179680592225296, 'rmse': 131.53522223840793, 'mape_pct': 65.27591639400785, 'r2': 0.08106266133295348}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RBF kernel, Linear SVR\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "inner_cv = TimeSeriesSplit(n_splits=3)\n",
        "cv = inner_cv\n",
        "\n",
        "rbf_svr = SVR(\n",
        "    kernel=\"rbf\",  # True RBF kernel\n",
        "    max_iter=64,   # Keep halving resource\n",
        "    cache_size=200  # MB for kernel cache\n",
        ")\n",
        "\n",
        "pipe_rbf = TransformedTargetRegressor(\n",
        "    regressor=Pipeline([\n",
        "        (\"shared\", shared_reg),\n",
        "        (\"svm\", rbf_svr),\n",
        "    ]),\n",
        "    func=np.log1p, inverse_func=np.expm1\n",
        ")\n",
        "\n",
        "grid_rbf = {\n",
        "    \"regressor__svm__C\":         [1e-2, 1e-1, 1, 10],\n",
        "    \"regressor__svm__epsilon\":   [0.05, 0.1],\n",
        "    \"regressor__svm__gamma\":     [1e-3, 1e-2, 1e-1],  # coarse log grid\n",
        "}\n",
        "\n",
        "hgs_rbf = HalvingGridSearchCV(\n",
        "    estimator=pipe_rbf,\n",
        "    param_grid=grid_rbf,\n",
        "    resource=\"regressor__svm__max_iter\",\n",
        "    min_resources=64, max_resources=512, factor=3,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=cv, n_jobs=NJOBS, verbose=1, refit=True\n",
        ")\n",
        "\n",
        "hgs_rbf.fit(X_train, y_train)\n",
        "rbf_pred = hgs_rbf.predict(X_test)\n",
        "rbf_metrics = evaluate_regression(y_test, rbf_pred, BASE_OUT, label=\"svm_rbf_nystroem\", dev_flag=DEVELOPMENT_MODE)\n",
        "print({\"svm_rbf_best\": hgs_rbf.best_params_, **rbf_metrics})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYBbP4AEEtwJ",
        "outputId": "ea592a4f-d21b-4fdb-b2f1-06666d723614"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_iterations: 2\n",
            "n_required_iterations: 3\n",
            "n_possible_iterations: 2\n",
            "min_resources_: 64\n",
            "max_resources_: 512\n",
            "aggressive_elimination: False\n",
            "factor: 3\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 24\n",
            "n_resources: 64\n",
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 8\n",
            "n_resources: 192\n",
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=192).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'svm_rbf_best': {'regressor__svm__C': 10, 'regressor__svm__epsilon': 0.05, 'regressor__svm__gamma': 0.001, 'regressor__svm__max_iter': 192}, 'mae': 64.16335475616454, 'medae': 30.451771918099155, 'rmse': 137.52876635964827, 'mape_pct': 67.62373098952742, 'r2': -0.004590038170670496}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Knn regressor\n",
        "\n",
        "from sklearn.experimental import enable_halving_search_cv  # noqa: F401\n",
        "from sklearn.model_selection import HalvingGridSearchCV\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.base import clone\n",
        "import numpy as np\n",
        "\n",
        "inner_cv = TimeSeriesSplit(n_splits=3)\n",
        "cv = inner_cv\n",
        "# --- Budget guard: k must be < train fold size ---\n",
        "approx_fold = int(len(X_train) * (inner_cv.get_n_splits()-1)/inner_cv.get_n_splits())  # ~ size of each training fold\n",
        "\n",
        "K_MIN = 64\n",
        "K_MAX_CAP = 256          # <-- our cap to avoid oversmoothing (was 512)\n",
        "# k must be < training fold size; also ensure K_MAX >= K_MIN+1\n",
        "K_MAX_SAFE = max(K_MIN + 1, min(K_MAX_CAP, approx_fold - 1))\n",
        "\n",
        "print(f\"KNN halving ladder: n_neighbors from {K_MIN} up to {K_MAX_SAFE} (factor=3)\")\n",
        "\n",
        "# Pipeline: shared sparse preprocessing -> KNN\n",
        "knn = KNeighborsRegressor(\n",
        "    algorithm=\"brute\",       # required for sparse + allows many metrics\n",
        "    metric=\"minkowski\",      # grid will try p=1/2\n",
        "    p=2,\n",
        "    weights=\"distance\",\n",
        "    n_jobs=NJOBS,\n",
        ")\n",
        "\n",
        "pipe_knn = TransformedTargetRegressor(\n",
        "    regressor=Pipeline([(\"shared\", shared_reg), (\"knn\", knn)]),\n",
        "    func=np.log1p, inverse_func=np.expm1\n",
        ")\n",
        "\n",
        "# Tiny, targeted grid (6 combos max)\n",
        "param_grid = {\n",
        "    \"regressor__knn__weights\": [\"uniform\", \"distance\"],\n",
        "    \"regressor__knn__metric\": [\"minkowski\"],  # keep it simple & robust on sparse\n",
        "    \"regressor__knn__p\": [1, 2],              # Manhattan vs Euclidean\n",
        "    # (If you want to try cosine later: add \"cosine\" to metric; keep algorithm=\"brute\")\n",
        "}\n",
        "\n",
        "hgs_knn = HalvingGridSearchCV(\n",
        "    estimator=pipe_knn,\n",
        "    param_grid=param_grid,\n",
        "    resource=\"regressor__knn__n_neighbors\",  # MATCHED BUDGET\n",
        "    min_resources=K_MIN, max_resources=K_MAX_SAFE, factor=3,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=cv, n_jobs=NJOBS, verbose=1, refit=True\n",
        ")\n",
        "\n",
        "hgs_knn.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate refit and (optionally) an ES-style “final” (not needed for KNN)\n",
        "pred_knn = hgs_knn.predict(X_test)\n",
        "metrics_knn = evaluate_regression(y_test, pred_knn, BASE_OUT,\n",
        "                                  label=\"knn_halving_refit\", dev_flag=DEVELOPMENT_MODE)\n",
        "\n",
        "print({\"knn_best_params\": hgs_knn.best_params_, **metrics_knn})\n",
        "\n",
        "# (Optional) print baseline right next to it\n",
        "train_median = float(np.median(y_train))\n",
        "baseline = evaluate_regression(y_test, np.full_like(y_test, train_median),\n",
        "                               BASE_OUT, label=\"baseline_median\", dev_flag=DEVELOPMENT_MODE)\n",
        "print({\"train_median_min\": train_median, **baseline})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISvfva_JE2k4",
        "outputId": "beee7be3-9446-4de7-ad94-be2ba9cfbd12"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN halving ladder: n_neighbors from 64 up to 256 (factor=3)\n",
            "n_iterations: 2\n",
            "n_required_iterations: 2\n",
            "n_possible_iterations: 2\n",
            "min_resources_: 64\n",
            "max_resources_: 256\n",
            "aggressive_elimination: False\n",
            "factor: 3\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 4\n",
            "n_resources: 64\n",
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 2\n",
            "n_resources: 192\n",
            "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
            "{'knn_best_params': {'regressor__knn__metric': 'minkowski', 'regressor__knn__p': 1, 'regressor__knn__weights': 'distance', 'regressor__knn__n_neighbors': 192}, 'mae': 61.2234807744056, 'medae': 25.36842232045066, 'rmse': 135.3166683817466, 'mape_pct': 62.567149437542746, 'r2': 0.027466957815660442}\n",
            "{'train_median_min': 60.0, 'mae': 67.88908742194471, 'medae': 30.283333333333335, 'rmse': 144.614580133217, 'mape_pct': 61.482678500772394, 'r2': -0.1107745854854012}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import clone\n",
        "\n",
        "best = hgs_knn.best_estimator_\n",
        "ks = [128, 160, 192, 224]   # around the winner\n",
        "results = {}\n",
        "for k in ks:\n",
        "    for w in [\"distance\", \"uniform\"]:\n",
        "        m = clone(best)\n",
        "        m.set_params(**{\"regressor__knn__n_neighbors\": k,\n",
        "                        \"regressor__knn__weights\": w})\n",
        "        m.fit(X_train, y_train)\n",
        "        pred = m.predict(X_test)\n",
        "        results[(k,w)] = evaluate_regression(\n",
        "            y_test, pred, BASE_OUT, label=f\"knn_k{k}_{w}\", dev_flag=DEVELOPMENT_MODE)\n",
        "\n",
        "# quick views\n",
        "print(\"Top by RMSE:\", sorted(results.items(), key=lambda kv: kv[1][\"rmse\"])[:3])\n",
        "print(\"Top by MAE :\", sorted(results.items(), key=lambda kv: kv[1][\"mae\"])[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60iRrF6KE6Rz",
        "outputId": "e9b29ddf-99b7-4385-ad85-31f97d3bd6a0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top by RMSE: [((224, 'uniform'), {'mae': 55.43095211679688, 'medae': 28.429933337987237, 'rmse': 119.51176247949664, 'mape_pct': 65.17928170455433, 'r2': -0.030413198391386898}), ((224, 'distance'), {'mae': 55.41548220920418, 'medae': 28.176861991754592, 'rmse': 119.5328904780959, 'mape_pct': 65.11309007661343, 'r2': -0.03077755571531382}), ((192, 'uniform'), {'mae': 55.587651323250284, 'medae': 28.458580469436896, 'rmse': 119.61672797351062, 'mape_pct': 65.3794477875074, 'r2': -0.03222398797243908})]\n",
            "Top by MAE : [((224, 'distance'), {'mae': 55.41548220920418, 'medae': 28.176861991754592, 'rmse': 119.5328904780959, 'mape_pct': 65.11309007661343, 'r2': -0.03077755571531382}), ((224, 'uniform'), {'mae': 55.43095211679688, 'medae': 28.429933337987237, 'rmse': 119.51176247949664, 'mape_pct': 65.17928170455433, 'r2': -0.030413198391386898}), ((192, 'distance'), {'mae': 55.56678513435276, 'medae': 28.561276742092748, 'rmse': 119.62873508110336, 'mape_pct': 65.31129428944827, 'r2': -0.03243122732346393})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ME8VX6ZmE82i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}