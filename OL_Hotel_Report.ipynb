{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPwJZCFy86GOeo4yQj5jxE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidGTeklea/BigIdeasFinal/blob/main/OL_Hotel_Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wsg1xjvgjgJM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8RiByDhb92c",
        "outputId": "de8e8119-f85c-4901-f816-6fe50c08fce4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.8.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.16.2)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (0.14.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.9.0->category_encoders) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n",
            "Downloading category_encoders-2.8.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.8.1\n"
          ]
        }
      ],
      "source": [
        "pip install category_encoders"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "yd9fTt058K2s",
        "outputId": "b5759065-595e-49b4-968a-dd92ebd2a37f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ee6a99ba-9eae-4501-81cb-dc838ae76b66\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ee6a99ba-9eae-4501-81cb-dc838ae76b66\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"davidteklea\",\"key\":\"6979f4292245affec7e1edf13a2bb74f\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n"
      ],
      "metadata": {
        "id": "xIX9HDMa8LGf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!kaggle datasets download -d jessemostipak/hotel-booking-demand -p data/\n",
        "!unzip data/hotel-booking-demand.zip -d data/\n"
      ],
      "metadata": {
        "id": "qR6n7oyCcGAF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e726c586-d334-4236-a268-39e21a6faf69"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.8.3)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.3)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n",
            "Dataset URL: https://www.kaggle.com/datasets/jessemostipak/hotel-booking-demand\n",
            "License(s): Attribution 4.0 International (CC BY 4.0)\n",
            "Downloading hotel-booking-demand.zip to data\n",
            "  0% 0.00/1.25M [00:00<?, ?B/s]\n",
            "100% 1.25M/1.25M [00:00<00:00, 704MB/s]\n",
            "Archive:  data/hotel-booking-demand.zip\n",
            "  inflating: data/hotel_bookings.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random, numpy as np, torch\n",
        "def set_seed(s=2025):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "    torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
        "set_seed(2025)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "GqYWNhsju2yv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Individual Time-Aware Split → Shared Preprocess\n",
        "# (low-card OHE, high-card CountEncoding, StandardScaler, float32)\n",
        "# ================================================================\n",
        "# python >=3.10\n",
        "# pip install scikit-learn>=1.2 category-encoders pandas numpy joblib matplotlib\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import os, time, pathlib, multiprocessing as mp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from joblib import Memory\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.base import clone\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "\n",
        "# ---------- CPU settings ----------\n",
        "NJOBS = min(2, mp.cpu_count() or 2)\n",
        "os.environ[\"OMP_NUM_THREADS\"] = str(NJOBS)\n",
        "os.environ[\"OPENBLAS_NUM_THREADS\"] = str(NJOBS)\n",
        "os.environ[\"MKL_NUM_THREADS\"] = str(NJOBS)\n",
        "os.environ[\"NUMEXPR_NUM_THREADS\"] = str(NJOBS)\n",
        "\n",
        "# ---------- Config ----------\n",
        "CSV_PATH   = \"data/hotel_bookings.csv\"\n",
        "TARGET     = \"is_canceled\"\n",
        "TIME_COL   = \"_t\"                  # helper booking timestamp (for splitting only)\n",
        "TEST_SIZE  = 0.20\n",
        "SEED_SPLIT = 2025\n",
        "\n",
        "# leakage drops (at-booking model)\n",
        "DROP_ALWAYS = [\"reservation_status\", \"reservation_status_date\"]\n",
        "# You can add more here later after EDA:\n",
        "DROP_OPTIONAL = []  # e.g., [\"booking_changes\",\"days_in_waiting_list\"]\n",
        "\n",
        "# high-cardinality cutoff and encoder settings\n",
        "HICARD_THRESH = 50\n",
        "COUNT_NORMALIZE = True  # frequency (0..1)\n",
        "\n",
        "# output/cache\n",
        "BASE_OUT = pathlib.Path(\"results/shared_pipeline_time_individual\")\n",
        "BASE_OUT.mkdir(parents=True, exist_ok=True)\n",
        "CACHE_DIR = pathlib.Path(\"cache_shared\")\n",
        "CACHE_DIR.mkdir(exist_ok=True)\n",
        "memory = Memory(location=str(CACHE_DIR), verbose=0)\n",
        "\n",
        "# ---------- Load ----------\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "df = df.drop(columns=[c for c in DROP_ALWAYS + DROP_OPTIONAL if c in df.columns]).copy()\n",
        "\n",
        "# ---------- Build booking_time = arrival_date - lead_time ----------\n",
        "def build_booking_time(d: pd.DataFrame) -> pd.Series:\n",
        "    # robust to month names\n",
        "    arr = pd.to_datetime(\n",
        "        d[\"arrival_date_year\"].astype(str) + \"-\" +\n",
        "        d[\"arrival_date_month\"].astype(str) + \"-\" +\n",
        "        d[\"arrival_date_day_of_month\"].astype(str),\n",
        "        errors=\"coerce\"\n",
        "    )\n",
        "    lead = pd.to_timedelta(d[\"lead_time\"].fillna(0).astype(int), unit=\"D\")\n",
        "    t = arr - lead                               # booking time (at booking)\n",
        "    if t.isna().all():\n",
        "        t = arr                                  # fallback to arrival\n",
        "    t = t.fillna(t.dropna().min())               # fill any remaining NaT\n",
        "    return t\n",
        "\n",
        "\n",
        "df[TIME_COL] = build_booking_time(df)\n",
        "min_ts = df[TIME_COL].dropna().min()\n",
        "df[TIME_COL] = df[TIME_COL].fillna(min_ts)\n",
        "\n",
        "\n",
        "# Add after df[TIME_COL] is created:\n",
        "X = df.drop(columns=[TARGET])\n",
        "y = df[TARGET].values\n",
        "# Time-based split\n",
        "split_time = X[TIME_COL].quantile(1 - TEST_SIZE)\n",
        "train_mask = X[TIME_COL] <= split_time\n",
        "X_train = X[train_mask].copy()\n",
        "X_test = X[~train_mask].copy()\n",
        "y_train = y[train_mask]\n",
        "y_test = y[~train_mask]\n",
        "w_train = compute_sample_weight(\n",
        "    class_weight='balanced',\n",
        "    y=y_train\n",
        ")\n",
        "# ---------- Sort TRAIN chronologically for TimeSeriesSplit ----------\n",
        "\n",
        "\n",
        "ord_tr = X_train[TIME_COL].argsort(kind=\"mergesort\")\n",
        "X_train = X_train.iloc[ord_tr].reset_index(drop=True)\n",
        "y_train = y_train[ord_tr]\n",
        "w_train = w_train[ord_tr]\n",
        "\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Shared preprocessing\n",
        "#  - Exclude TIME_COL from features (split helper; seasonality should come from arrival parts)\n",
        "#  - numeric: impute → (global) StandardScaler(with_mean=False)\n",
        "#  - categorical low-card: OHE (sparse)\n",
        "#  - categorical high-card: CountEncoder (dense) → CSR\n",
        "#  - cast to float32\n",
        "# ================================================================\n",
        "def dense_to_csr(X):\n",
        "    if sp.issparse(X): return X.tocsr()\n",
        "    return sp.csr_matrix(X)\n",
        "\n",
        "def to_dense(X):\n",
        "    import numpy as np, scipy.sparse as sp\n",
        "    return X.toarray() if sp.issparse(X) else np.asarray(X)\n",
        "\n",
        "def to_float32(X):\n",
        "    import numpy as np, scipy.sparse as sp\n",
        "    if sp.issparse(X): return X.astype(np.float32)\n",
        "    return np.asarray(X, dtype=np.float32, copy=False)\n",
        "\n",
        "def make_shared_preprocess(X_frame: pd.DataFrame, memory=None):\n",
        "    Xf = X_frame.drop(columns=[TIME_COL], errors=\"ignore\")\n",
        "    cat_cols = [c for c in Xf.columns if Xf[c].dtype == \"object\"]\n",
        "    num_cols = [c for c in Xf.columns if c not in cat_cols]\n",
        "\n",
        "    nunique = {c: Xf[c].nunique(dropna=True) for c in cat_cols}\n",
        "    cat_low = [c for c in cat_cols if nunique[c] <= HICARD_THRESH]\n",
        "    cat_hi  = [c for c in cat_cols if nunique[c] >  HICARD_THRESH]\n",
        "\n",
        "    num_pipe = Pipeline([\n",
        "        (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"sc\",  StandardScaler()),\n",
        "    ], memory=memory)  # <- no caching\n",
        "\n",
        "    low_pipe = Pipeline([\n",
        "        (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\",\n",
        "                              sparse_output=True, dtype=np.float32)),\n",
        "    ], memory=memory)  # <- no caching\n",
        "\n",
        "    # No csr step; keep it dense here, union stays sparse due to OHE + sparse_threshold\n",
        "    hi_pipe = Pipeline([\n",
        "        (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"cnt\", ce.CountEncoder(normalize=COUNT_NORMALIZE)),\n",
        "        (\"sc\",  StandardScaler()),\n",
        "    ], memory=memory)  # <- no caching\n",
        "\n",
        "    preprocess = ColumnTransformer([\n",
        "        (\"num\",     num_pipe, num_cols),\n",
        "        (\"cat_low\", low_pipe, cat_low),\n",
        "        (\"cat_hi\",  hi_pipe,  cat_hi),\n",
        "    ], remainder=\"drop\", sparse_threshold=1.0, n_jobs=None)  # <- no n_jobs inside\n",
        "\n",
        "    shared = Pipeline([\n",
        "        (\"prep\",  preprocess),\n",
        "        (\"fp32\",  FunctionTransformer(to_float32, accept_sparse=True, validate=False)),\n",
        "    ], memory=memory)  # <- no caching\n",
        "\n",
        "    return shared\n",
        "\n",
        "def add_densify(shared_pipeline):\n",
        "    steps = list(shared_pipeline.steps) + [\n",
        "        (\"dense\", FunctionTransformer(to_dense, accept_sparse=True, validate=False)),\n",
        "        (\"fp32b\", FunctionTransformer(to_float32, accept_sparse=False, validate=False)),\n",
        "    ]\n",
        "    return Pipeline(steps, memory=memory)  # <- no caching\n",
        "\n",
        "shared_pre = make_shared_preprocess(X_train, memory)\n",
        "\n",
        "# 1) Fit a *throwaway* copy of your preprocessor just to read its width. for rbf\n",
        "tmp_pre = make_shared_preprocess(X_train)\n",
        "tmp_pre.fit(X_train)                           # train-split only; no leakage to test\n",
        "d = tmp_pre.transform(X_train.iloc[:1]).shape[1]\n",
        "print(d)\n",
        "\n",
        "\n",
        "# ---------- Time-aware CV (individual, forward-chaining) ----------\n",
        "INNER_SPLITS = 3\n",
        "INNER_CV = TimeSeriesSplit(n_splits=INNER_SPLITS)\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_curve, f1_score, confusion_matrix\n",
        "\n",
        "def choose_threshold_max_f1(y_true, scores, pos_label=1):\n",
        "    \"\"\"Return t*, best_F1, precision_at_t*, recall_at_t* using decision_function scores.\"\"\"\n",
        "    P, R, T = precision_recall_curve(y_true, scores, pos_label=pos_label)\n",
        "    # precision_recall_curve returns len(T) = len(P) - 1 = len(R) - 1\n",
        "    f1 = 2 * P[1:] * R[1:] / (P[1:] + R[1:] + 1e-12)\n",
        "    i = np.nanargmax(f1)\n",
        "    return T[i], float(f1[i]), float(P[i+1]), float(R[i+1])\n",
        "\n",
        "def apply_threshold(scores, t_star):\n",
        "    return (scores >= t_star).astype(int)\n",
        "\n",
        "TAUS = globals().get(\"TAUS\", {})\n",
        "\n"
      ],
      "metadata": {
        "id": "mstZNSNE2744",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baa1615a-d66c-4378-98f7-3d4354abf160"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install skorch"
      ],
      "metadata": {
        "id": "tVmAhC6E4ke6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6332aff8-9106-462a-d95b-944bed34f4f0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting skorch\n",
            "  Downloading skorch-1.2.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from skorch) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from skorch) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from skorch) (1.16.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.12/dist-packages (from skorch) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.12/dist-packages (from skorch) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.0->skorch) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.0->skorch) (3.6.0)\n",
            "Downloading skorch-1.2.0-py3-none-any.whl (263 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.1/263.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: skorch\n",
            "Successfully installed skorch-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Make a chronological validation split *inside your training period*\n",
        "VAL_SIZE = 0.20\n",
        "cut = X_train[TIME_COL].quantile(1 - VAL_SIZE)\n",
        "\n",
        "tr_mask  = X_train[TIME_COL] <= cut      # earlier 80% -> model-fitting \"train\"\n",
        "val_mask = ~tr_mask                      # later 20%  -> fixed \"validation\"\n",
        "\n",
        "X_tr_df  = X_train.loc[tr_mask].copy()\n",
        "y_tr     = y_train[tr_mask]\n",
        "X_val_df = X_train.loc[val_mask].copy()\n",
        "y_val    = y_train[val_mask]\n",
        "\n",
        "# 2) Fit your existing preprocessor on TRAIN ONLY (no leakage)\n",
        "shared_pre = make_shared_preprocess(X_tr_df, memory=None)\n",
        "shared_pre.fit(X_tr_df)                  # <— this is what I meant by \"shared_pre_fit\"\n",
        "\n",
        "# 3) Transform to numpy (float32). If sparse, densify before tensors.\n",
        "import numpy as np, scipy.sparse as sp\n",
        "def to_dense_np(X):\n",
        "    return X.toarray().astype(np.float32) if sp.issparse(X) else np.asarray(X, np.float32)\n",
        "\n",
        "X_tr_np  = to_dense_np(shared_pre.transform(X_tr_df))\n",
        "X_val_np = to_dense_np(shared_pre.transform(X_val_df))\n",
        "X_te_np  = to_dense_np(shared_pre.transform(X_test.copy()))\n",
        "y_tr_np  = y_tr.astype(np.float32)\n",
        "y_val_np = y_val.astype(np.float32)\n",
        "y_te_np  = y_test.astype(np.float32)\n",
        "\n",
        "tr = torch.from_numpy(X_tr_np).float();\n",
        "ytr = torch.from_numpy(y_tr_np).float();\n",
        "\n",
        "# --- continue from your last line ---\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# 4) Convert to tensors (binary classification → float targets)\n",
        "Xtr_t  = torch.from_numpy(X_tr_np).float()\n",
        "Xval_t = torch.from_numpy(X_val_np).float()\n",
        "Xte_t  = torch.from_numpy(X_te_np).float()\n",
        "\n",
        "ytr_t  = torch.from_numpy(y_tr_np).float()   # 0/1 as float for BCEWithLogitsLoss\n",
        "yval_t = torch.from_numpy(y_val_np).float()\n",
        "yte_t  = torch.from_numpy(y_te_np).float()\n",
        "\n",
        "# 5) Wrap into datasets/loaders (shuffle only on training)\n",
        "train_loader = DataLoader(TensorDataset(Xtr_t,  ytr_t),  batch_size=256,  shuffle=True,  drop_last=False)\n",
        "val_loader   = DataLoader(TensorDataset(Xval_t, yval_t), batch_size=1024, shuffle=False)\n",
        "test_loader  = DataLoader(TensorDataset(Xte_t,  yte_t),  batch_size=1024, shuffle=False)\n",
        "\n",
        "# --- quick sanity checks (optional) ---\n",
        "xb, yb = next(iter(train_loader))\n",
        "print(\"Train batch:\", xb.shape, yb.shape, xb.dtype, yb.dtype)  # e.g., [256, d], [256]\n",
        "print(\"Val size:\", len(val_loader.dataset), \"Test size:\", len(test_loader.dataset))\n",
        "\n"
      ],
      "metadata": {
        "id": "RXjJv6mNqCCC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e9be217-67a9-49cb-9618-63fd3ac68ba7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batch: torch.Size([256, 81]) torch.Size([256]) torch.float32 torch.float32\n",
            "Val size: 19108 Test size: 23703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== MLP (skorch, SGD) — Halving v2 (balanced, faster learning) ===================== #with treshold\n",
        "import numpy as np, torch, torch.nn as nn\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, precision_recall_curve  # <- added PRC\n",
        "from sklearn.experimental import enable_halving_search_cv  # noqa: F401\n",
        "from sklearn.model_selection import HalvingGridSearchCV, TimeSeriesSplit\n",
        "from sklearn.base import clone  # <- added\n",
        "from skorch import NeuralNetBinaryClassifier\n",
        "from skorch.callbacks import EarlyStopping\n",
        "\n",
        "# imbalance weight (pos_weight = neg/pos)\n",
        "pos = float((y_train == 1).sum())\n",
        "neg = float((y_train == 0).sum())\n",
        "pos_weight = torch.tensor([neg / max(pos, 1.0)], dtype=torch.float32)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hidden=(512, 512), dropout=0.0):\n",
        "        super().__init__()\n",
        "        layers, lazy, prev = [], True, None\n",
        "        for h in hidden:\n",
        "            layers += [nn.LazyLinear(h) if lazy else nn.Linear(prev, h), nn.ReLU()]\n",
        "            if dropout > 0: layers += [nn.Dropout(dropout)]\n",
        "            prev, lazy = h, False\n",
        "        layers += [nn.Linear(prev, 1)]\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self, X):\n",
        "        return self.net(X).squeeze(-1)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "net = NeuralNetBinaryClassifier(\n",
        "    module=MLP,\n",
        "    max_epochs=15,\n",
        "    optimizer=torch.optim.SGD,\n",
        "    optimizer__momentum=0.0, optimizer__nesterov=False,\n",
        "    criterion=nn.BCEWithLogitsLoss,\n",
        "    criterion__pos_weight=pos_weight,\n",
        "    batch_size=2048,\n",
        "    iterator_train__shuffle=True,\n",
        "    iterator_train__num_workers=0,\n",
        "    iterator_valid__num_workers=0,\n",
        "    iterator_train__pin_memory=False,\n",
        "    iterator_valid__pin_memory=False,\n",
        "    train_split=None,\n",
        "    callbacks=[EarlyStopping(monitor='train_loss', patience=2, threshold=1e-4)],\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "pipe_nn = Pipeline([\n",
        "    (\"pre\", shared_pre),\n",
        "    (\"clf\", net),\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    \"clf__module__hidden\": [(1024, 128), (216, 216, 216, 216, 216)],\n",
        "    \"clf__optimizer__lr\": [0.1, 0.5, 1],\n",
        "    \"clf__optimizer__weight_decay\": [1e-4, 1e-3],\n",
        "    \"clf__batch_size\": [2048],\n",
        "}\n",
        "\n",
        "n_splits = 3\n",
        "\n",
        "# for just 10,000 rows speed test\n",
        "# X_train = X_train.iloc[:10000]\n",
        "# y_train = y_train[:10000]\n",
        "\n",
        "INNER_CV = TimeSeriesSplit(n_splits=n_splits)\n",
        "N_TRAIN = len(X_train)\n",
        "min_train_fold = max(2048, N_TRAIN // (n_splits + 2))\n",
        "\n",
        "hgrid_nn = HalvingGridSearchCV(\n",
        "    pipe_nn,\n",
        "    param_grid=param_grid,\n",
        "    cv=INNER_CV,\n",
        "    scoring=\"roc_auc\",\n",
        "    resource=\"n_samples\",\n",
        "    factor=4,\n",
        "    min_resources=min_train_fold,\n",
        "    max_resources=N_TRAIN,\n",
        "    aggressive_elimination=True,\n",
        "    n_jobs=1,            # IMPORTANT with skorch\n",
        "    refit=True,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "hgrid_nn.fit(X_train, y_train.astype(np.float32))\n",
        "\n",
        "# ===== Metrics =====\n",
        "def proba(est, X):\n",
        "    p = est.predict_proba(X)\n",
        "    return p.ravel() if p.ndim == 2 and p.shape[1] == 1 else p[:, 1]\n",
        "\n",
        "# ---- pick τ* on LAST fold (time-aware), using predict_proba ----\n",
        "tr_idx, val_idx = list(INNER_CV.split(X_train))[-1]\n",
        "nn_lastfold = clone(hgrid_nn.best_estimator_).fit(X_train.iloc[tr_idx], y_train[tr_idx].astype(np.float32))\n",
        "val_scores = proba(nn_lastfold, X_train.iloc[val_idx])\n",
        "P, R, T = precision_recall_curve(y_train[val_idx], val_scores)\n",
        "F1 = 2 * P * R / (P + R + 1e-12)\n",
        "tau_star = float(T[F1[:-1].argmax()])\n",
        "print(f\"τ* (last-fold, max F1): {tau_star:.4f}\")\n",
        "\n",
        "p_tr = proba(hgrid_nn, X_train)\n",
        "p_te = proba(hgrid_nn, X_test)\n",
        "\n",
        "best_net = hgrid_nn.best_estimator_.named_steps[\"clf\"]\n",
        "param_count = int(sum(p.numel() for p in best_net.module_.parameters() if p.requires_grad))\n",
        "epochs = len(best_net.history_) if hasattr(best_net, \"history_\") else None\n",
        "\n",
        "print(\"\\n=== MLP (skorch, SGD) — Halving v2 ===\")\n",
        "print(\"best params:\", hgrid_nn.best_params_)\n",
        "print(\"param count:\", param_count, \"| epochs:\", epochs)\n",
        "print(\"best CV ROC-AUC:\", round(hgrid_nn.best_score_, 4))\n",
        "print(\"train ROC-AUC:\", round(roc_auc_score(y_train, p_tr), 4))\n",
        "print(\"test  ROC-AUC:\",  round(roc_auc_score(y_test,  p_te), 4))\n",
        "print(\"train acc @τ*:\", round(accuracy_score(y_train, (p_tr >= tau_star).astype(int)), 4))  # <- updated\n",
        "print(\"test  acc @τ*:\",  round(accuracy_score(y_test,  (p_te >= tau_star).astype(int)), 4))  # <- updated\n",
        "print(\"train AP:\", round(average_precision_score(y_train, p_tr), 4))\n",
        "print(\"test  AP:\",  round(average_precision_score(y_test,  p_te), 4))\n",
        "\n",
        "TAUS[\"MLP (SGD)\"]    = tau_star\n"
      ],
      "metadata": {
        "id": "GEpTOE7crvBP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bcac3b7-46b9-4998-ce1f-28ed4fd7c120"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_iterations: 2\n",
            "n_required_iterations: 2\n",
            "n_possible_iterations: 2\n",
            "min_resources_: 19137\n",
            "max_resources_: 95687\n",
            "aggressive_elimination: True\n",
            "factor: 4\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 12\n",
            "n_resources: 19137\n",
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8711\u001b[0m  1.8623\n",
            "      2        \u001b[36m0.8444\u001b[0m  2.4384\n",
            "      3        \u001b[36m0.8157\u001b[0m  2.4024\n",
            "      4        \u001b[36m0.7813\u001b[0m  1.2280\n",
            "      5        \u001b[36m0.7399\u001b[0m  2.2592\n",
            "      6        \u001b[36m0.6934\u001b[0m  3.3241\n",
            "      7        \u001b[36m0.6436\u001b[0m  2.5230\n",
            "      8        \u001b[36m0.5947\u001b[0m  2.9907\n",
            "      9        \u001b[36m0.5492\u001b[0m  1.8695\n",
            "     10        \u001b[36m0.5081\u001b[0m  1.4514\n",
            "     11        \u001b[36m0.4726\u001b[0m  1.6288\n",
            "     12        \u001b[36m0.4441\u001b[0m  0.9430\n",
            "     13        \u001b[36m0.4193\u001b[0m  0.4726\n",
            "     14        \u001b[36m0.3995\u001b[0m  0.5203\n",
            "     15        \u001b[36m0.3830\u001b[0m  0.4751\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8453\u001b[0m  0.9015\n",
            "      2        \u001b[36m0.7964\u001b[0m  0.9104\n",
            "      3        \u001b[36m0.7370\u001b[0m  0.9270\n",
            "      4        \u001b[36m0.6687\u001b[0m  1.2253\n",
            "      5        \u001b[36m0.6018\u001b[0m  1.2389\n",
            "      6        \u001b[36m0.5466\u001b[0m  0.8390\n",
            "      7        \u001b[36m0.5049\u001b[0m  0.8560\n",
            "      8        \u001b[36m0.4750\u001b[0m  0.8859\n",
            "      9        \u001b[36m0.4534\u001b[0m  0.8937\n",
            "     10        \u001b[36m0.4369\u001b[0m  0.8642\n",
            "     11        \u001b[36m0.4246\u001b[0m  0.8231\n",
            "     12        \u001b[36m0.4149\u001b[0m  0.8925\n",
            "     13        \u001b[36m0.4061\u001b[0m  0.9543\n",
            "     14        \u001b[36m0.3993\u001b[0m  0.8166\n",
            "     15        \u001b[36m0.3934\u001b[0m  0.8404\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8307\u001b[0m  1.9260\n",
            "      2        \u001b[36m0.7730\u001b[0m  1.2771\n",
            "      3        \u001b[36m0.7072\u001b[0m  1.3782\n",
            "      4        \u001b[36m0.6378\u001b[0m  1.2600\n",
            "      5        \u001b[36m0.5885\u001b[0m  1.2988\n",
            "      6        \u001b[36m0.5456\u001b[0m  1.3106\n",
            "      7        \u001b[36m0.5230\u001b[0m  1.2521\n",
            "      8        \u001b[36m0.5111\u001b[0m  1.2611\n",
            "      9        \u001b[36m0.4897\u001b[0m  1.3717\n",
            "     10        \u001b[36m0.4894\u001b[0m  1.8651\n",
            "     11        \u001b[36m0.4766\u001b[0m  1.3606\n",
            "     12        \u001b[36m0.4738\u001b[0m  1.2617\n",
            "     13        \u001b[36m0.4650\u001b[0m  1.2716\n",
            "     14        \u001b[36m0.4605\u001b[0m  1.2791\n",
            "     15        0.4671  1.2416\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8678\u001b[0m  0.4123\n",
            "      2        \u001b[36m0.8413\u001b[0m  0.5182\n",
            "      3        \u001b[36m0.8127\u001b[0m  0.4389\n",
            "      4        \u001b[36m0.7781\u001b[0m  0.4096\n",
            "      5        \u001b[36m0.7359\u001b[0m  0.5860\n",
            "      6        \u001b[36m0.6879\u001b[0m  0.6256\n",
            "      7        \u001b[36m0.6373\u001b[0m  0.6137\n",
            "      8        \u001b[36m0.5870\u001b[0m  0.6379\n",
            "      9        \u001b[36m0.5410\u001b[0m  0.6278\n",
            "     10        \u001b[36m0.5009\u001b[0m  0.4517\n",
            "     11        \u001b[36m0.4670\u001b[0m  0.4401\n",
            "     12        \u001b[36m0.4381\u001b[0m  0.4470\n",
            "     13        \u001b[36m0.4152\u001b[0m  0.5105\n",
            "     14        \u001b[36m0.3974\u001b[0m  0.4122\n",
            "     15        \u001b[36m0.3819\u001b[0m  0.4254\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8465\u001b[0m  0.8224\n",
            "      2        \u001b[36m0.8033\u001b[0m  0.8593\n",
            "      3        \u001b[36m0.7500\u001b[0m  0.8207\n",
            "      4        \u001b[36m0.6858\u001b[0m  0.9118\n",
            "      5        \u001b[36m0.6196\u001b[0m  1.0411\n",
            "      6        \u001b[36m0.5626\u001b[0m  0.8663\n",
            "      7        \u001b[36m0.5180\u001b[0m  0.8980\n",
            "      8        \u001b[36m0.4853\u001b[0m  1.2564\n",
            "      9        \u001b[36m0.4610\u001b[0m  1.2597\n",
            "     10        \u001b[36m0.4431\u001b[0m  0.8650\n",
            "     11        \u001b[36m0.4295\u001b[0m  0.8521\n",
            "     12        \u001b[36m0.4186\u001b[0m  0.8076\n",
            "     13        \u001b[36m0.4102\u001b[0m  0.8353\n",
            "     14        \u001b[36m0.4036\u001b[0m  0.8675\n",
            "     15        \u001b[36m0.3974\u001b[0m  0.8527\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8432\u001b[0m  1.2319\n",
            "      2        \u001b[36m0.8038\u001b[0m  1.2476\n",
            "      3        \u001b[36m0.7440\u001b[0m  1.3740\n",
            "      4        \u001b[36m0.6729\u001b[0m  1.9133\n",
            "      5        \u001b[36m0.6100\u001b[0m  1.3708\n",
            "      6        \u001b[36m0.5714\u001b[0m  1.2319\n",
            "      7        \u001b[36m0.5476\u001b[0m  1.3278\n",
            "      8        \u001b[36m0.5136\u001b[0m  1.2882\n",
            "      9        \u001b[36m0.4977\u001b[0m  1.3320\n",
            "     10        \u001b[36m0.4834\u001b[0m  1.2433\n",
            "     11        0.4914  1.2645\n",
            "     12        \u001b[36m0.4697\u001b[0m  1.3344\n",
            "     13        0.4697  1.9100\n",
            "     14        \u001b[36m0.4554\u001b[0m  1.5799\n",
            "     15        \u001b[36m0.4552\u001b[0m  1.2738\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8364\u001b[0m  0.4630\n",
            "      2        \u001b[36m0.6466\u001b[0m  0.5143\n",
            "      3        0.8211  0.4295\n",
            "      4        \u001b[36m0.5925\u001b[0m  0.4739\n",
            "      5        \u001b[36m0.4140\u001b[0m  0.4756\n",
            "      6        \u001b[36m0.3448\u001b[0m  0.5537\n",
            "      7        \u001b[36m0.3225\u001b[0m  0.5099\n",
            "      8        0.3571  0.4906\n",
            "Stopping since train_loss has not improved in the last 2 epochs.\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.7902\u001b[0m  1.1747\n",
            "      2        \u001b[36m0.7532\u001b[0m  1.2475\n",
            "      3        \u001b[36m0.6036\u001b[0m  1.9051\n",
            "      4        \u001b[36m0.4392\u001b[0m  1.5677\n",
            "      5        0.4513  0.8089\n",
            "      6        \u001b[36m0.4163\u001b[0m  0.8703\n",
            "      7        \u001b[36m0.3871\u001b[0m  0.8461\n",
            "      8        0.3939  0.8849\n",
            "      9        \u001b[36m0.3844\u001b[0m  0.8433\n",
            "     10        \u001b[36m0.3603\u001b[0m  0.8331\n",
            "     11        \u001b[36m0.3454\u001b[0m  0.8567\n",
            "     12        0.3467  0.8880\n",
            "Stopping since train_loss has not improved in the last 2 epochs.\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.7688\u001b[0m  1.3574\n",
            "      2        \u001b[36m0.7151\u001b[0m  1.3322\n",
            "      3        \u001b[36m0.5578\u001b[0m  1.2470\n",
            "      4        0.7044  2.4302\n",
            "      5        \u001b[36m0.4683\u001b[0m  2.7536\n",
            "      6        0.5282  1.4743\n",
            "Stopping since train_loss has not improved in the last 2 epochs.\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8411\u001b[0m  0.4223\n",
            "      2        \u001b[36m0.6590\u001b[0m  0.4177\n",
            "      3        \u001b[36m0.6513\u001b[0m  0.4325\n",
            "      4        0.7421  0.4570\n",
            "      5        \u001b[36m0.5317\u001b[0m  0.4545\n",
            "      6        \u001b[36m0.4158\u001b[0m  0.4511\n",
            "      7        \u001b[36m0.3437\u001b[0m  0.4423\n",
            "      8        \u001b[36m0.3170\u001b[0m  0.4957\n",
            "      9        0.3675  0.4381\n",
            "Stopping since train_loss has not improved in the last 2 epochs.\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.7817\u001b[0m  0.8348\n",
            "      2        \u001b[36m0.7307\u001b[0m  1.0442\n",
            "      3        \u001b[36m0.5335\u001b[0m  0.8302\n",
            "      4        \u001b[36m0.4284\u001b[0m  1.2210\n",
            "      5        \u001b[36m0.4023\u001b[0m  1.2580\n",
            "      6        0.5807  1.0149\n",
            "      7        \u001b[36m0.3869\u001b[0m  0.8999\n",
            "      8        \u001b[36m0.3690\u001b[0m  0.8289\n",
            "      9        \u001b[36m0.3644\u001b[0m  0.8910\n",
            "     10        \u001b[36m0.3527\u001b[0m  0.9253\n",
            "     11        \u001b[36m0.3495\u001b[0m  0.9386\n",
            "     12        0.3804  0.8707\n",
            "     13        \u001b[36m0.3308\u001b[0m  0.8550\n",
            "     14        0.3500  0.8458\n",
            "     15        \u001b[36m0.3292\u001b[0m  0.8514\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.7660\u001b[0m  1.8710\n",
            "      2        \u001b[36m0.7320\u001b[0m  1.4369\n",
            "      3        \u001b[36m0.5525\u001b[0m  1.3276\n",
            "      4        \u001b[36m0.5484\u001b[0m  1.3124\n",
            "      5        \u001b[36m0.4784\u001b[0m  1.2961\n",
            "      6        0.5731  1.2435\n",
            "      7        \u001b[36m0.4459\u001b[0m  1.2687\n",
            "      8        0.4460  1.2278\n",
            "Stopping since train_loss has not improved in the last 2 epochs.\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.7960\u001b[0m  0.5004\n",
            "      2        1.4422  0.4883\n",
            "      3        \u001b[36m0.7770\u001b[0m  0.4316\n",
            "      4        \u001b[36m0.5478\u001b[0m  0.4775\n",
            "      5        \u001b[36m0.3926\u001b[0m  0.4485\n",
            "      6        1.2178  0.5085\n",
            "Stopping since train_loss has not improved in the last 2 epochs.\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.7883\u001b[0m  0.8290\n",
            "      2        1.2933  0.8534\n",
            "      3        \u001b[36m0.7743\u001b[0m  1.0518\n",
            "      4        \u001b[36m0.7239\u001b[0m  0.8297\n",
            "      5        \u001b[36m0.4674\u001b[0m  0.9446\n",
            "      6        \u001b[36m0.4213\u001b[0m  0.8645\n",
            "      7        0.4404  1.0976\n",
            "      8        \u001b[36m0.3649\u001b[0m  1.2570\n",
            "      9        \u001b[36m0.3443\u001b[0m  1.0829\n",
            "     10        0.3945  0.8572\n",
            "Stopping since train_loss has not improved in the last 2 epochs.\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9115\u001b[0m  1.2535\n",
            "      2        \u001b[36m0.8241\u001b[0m  1.2501\n",
            "      3        \u001b[36m0.7697\u001b[0m  1.2410\n",
            "      4        0.9028  1.2706\n",
            "Stopping since train_loss has not improved in the last 2 epochs.\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8198\u001b[0m  0.5039\n",
            "      2        1.1623  0.4180\n",
            "      3        \u001b[36m0.6188\u001b[0m  0.3992\n",
            "      4        \u001b[36m0.4454\u001b[0m  0.4568\n",
            "      5        0.5917  0.4593\n",
            "Stopping since train_loss has not improved in the last 2 epochs.\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.7084\u001b[0m  0.8777\n",
            "      2        1.2689  0.8854\n",
            "      3        \u001b[36m0.5836\u001b[0m  0.9081\n",
            "      4        0.8831  0.9052\n",
            "      5        \u001b[36m0.5160\u001b[0m  0.8778\n",
            "      6        \u001b[36m0.3847\u001b[0m  0.8617\n",
            "      7        0.4409  0.9073\n",
            "      8        \u001b[36m0.3618\u001b[0m  1.1696\n",
            "      9        \u001b[36m0.3349\u001b[0m  1.2662\n",
            "     10        \u001b[36m0.3249\u001b[0m  0.9639\n",
            "     11        0.3857  0.8440\n",
            "     12        \u001b[36m0.3152\u001b[0m  0.8852\n",
            "     13        0.3409  0.8983\n",
            "Stopping since train_loss has not improved in the last 2 epochs.\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9429\u001b[0m  1.2441\n",
            "      2        \u001b[36m0.7977\u001b[0m  1.3080\n",
            "      3        \u001b[36m0.7975\u001b[0m  1.2430\n",
            "      4        \u001b[36m0.6652\u001b[0m  1.6410\n",
            "      5        \u001b[36m0.6029\u001b[0m  1.7761\n",
            "      6        \u001b[36m0.5392\u001b[0m  1.2446\n",
            "      7        0.7552  1.2366\n",
            "      8        \u001b[36m0.5205\u001b[0m  1.2504\n",
            "      9        \u001b[36m0.4386\u001b[0m  1.2721\n",
            "     10        0.4705  1.2902\n",
            "     11        \u001b[36m0.4289\u001b[0m  1.3148\n",
            "     12        0.4325  1.2886\n",
            "Stopping since train_loss has not improved in the last 2 epochs.\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8716\u001b[0m  0.4066\n",
            "      2        \u001b[36m0.8705\u001b[0m  0.4385\n",
            "      3        \u001b[36m0.8697\u001b[0m  0.4088\n",
            "      4        \u001b[36m0.8690\u001b[0m  0.4422\n",
            "      5        \u001b[36m0.8684\u001b[0m  0.4098\n",
            "      6        \u001b[36m0.8680\u001b[0m  0.4476\n",
            "      7        \u001b[36m0.8676\u001b[0m  0.5024\n",
            "      8        \u001b[36m0.8672\u001b[0m  0.4228\n",
            "      9        \u001b[36m0.8669\u001b[0m  0.4919\n",
            "     10        \u001b[36m0.8666\u001b[0m  0.4373\n",
            "     11        \u001b[36m0.8663\u001b[0m  0.4905\n",
            "     12        \u001b[36m0.8661\u001b[0m  0.4938\n",
            "     13        \u001b[36m0.8658\u001b[0m  0.4747\n",
            "     14        \u001b[36m0.8655\u001b[0m  0.4389\n",
            "     15        \u001b[36m0.8653\u001b[0m  0.4528\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8602\u001b[0m  0.8156\n",
            "      2        \u001b[36m0.8600\u001b[0m  0.8361\n",
            "      3        \u001b[36m0.8598\u001b[0m  1.2742\n",
            "      4        \u001b[36m0.8596\u001b[0m  1.2995\n",
            "      5        \u001b[36m0.8594\u001b[0m  0.8753\n",
            "      6        \u001b[36m0.8592\u001b[0m  0.8679\n",
            "      7        \u001b[36m0.8591\u001b[0m  0.8287\n",
            "      8        \u001b[36m0.8589\u001b[0m  0.9232\n",
            "      9        \u001b[36m0.8587\u001b[0m  1.1586\n",
            "     10        \u001b[36m0.8585\u001b[0m  1.2821\n",
            "     11        \u001b[36m0.8583\u001b[0m  1.3641\n",
            "     12        \u001b[36m0.8580\u001b[0m  1.5922\n",
            "     13        \u001b[36m0.8578\u001b[0m  1.0942\n",
            "     14        \u001b[36m0.8575\u001b[0m  1.2803\n",
            "     15        \u001b[36m0.8572\u001b[0m  1.6015\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8542\u001b[0m  1.2325\n",
            "      2        \u001b[36m0.8540\u001b[0m  1.3208\n",
            "      3        \u001b[36m0.8538\u001b[0m  1.2248\n",
            "      4        \u001b[36m0.8537\u001b[0m  1.2341\n",
            "      5        0.8537  1.2405\n",
            "      6        \u001b[36m0.8534\u001b[0m  1.2531\n",
            "      7        \u001b[36m0.8533\u001b[0m  1.3320\n",
            "      8        \u001b[36m0.8530\u001b[0m  1.8821\n",
            "      9        0.8530  1.3985\n",
            "     10        \u001b[36m0.8527\u001b[0m  1.2410\n",
            "     11        \u001b[36m0.8524\u001b[0m  1.2887\n",
            "     12        \u001b[36m0.8522\u001b[0m  1.2854\n",
            "     13        \u001b[36m0.8520\u001b[0m  1.2840\n",
            "     14        \u001b[36m0.8517\u001b[0m  1.2054\n",
            "     15        \u001b[36m0.8514\u001b[0m  1.2832\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8712\u001b[0m  0.6398\n",
            "      2        \u001b[36m0.8702\u001b[0m  0.6524\n",
            "      3        \u001b[36m0.8694\u001b[0m  0.6424\n",
            "      4        \u001b[36m0.8688\u001b[0m  0.5085\n",
            "      5        \u001b[36m0.8684\u001b[0m  0.4316\n",
            "      6        \u001b[36m0.8680\u001b[0m  0.4518\n",
            "      7        \u001b[36m0.8675\u001b[0m  0.4268\n",
            "      8        \u001b[36m0.8673\u001b[0m  0.4512\n",
            "      9        \u001b[36m0.8670\u001b[0m  0.4457\n",
            "     10        \u001b[36m0.8668\u001b[0m  0.4437\n",
            "     11        \u001b[36m0.8666\u001b[0m  0.4190\n",
            "     12        \u001b[36m0.8663\u001b[0m  0.4449\n",
            "     13        \u001b[36m0.8661\u001b[0m  0.4601\n",
            "     14        \u001b[36m0.8659\u001b[0m  0.4799\n",
            "     15        \u001b[36m0.8657\u001b[0m  0.5185\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8603\u001b[0m  0.8306\n",
            "      2        \u001b[36m0.8600\u001b[0m  0.8794\n",
            "      3        \u001b[36m0.8599\u001b[0m  0.8599\n",
            "      4        \u001b[36m0.8597\u001b[0m  0.8679\n",
            "      5        \u001b[36m0.8596\u001b[0m  1.1153\n",
            "      6        \u001b[36m0.8595\u001b[0m  1.2347\n",
            "      7        \u001b[36m0.8594\u001b[0m  1.0513\n",
            "      8        \u001b[36m0.8593\u001b[0m  1.0361\n",
            "      9        \u001b[36m0.8591\u001b[0m  0.8969\n",
            "     10        \u001b[36m0.8590\u001b[0m  0.8971\n",
            "     11        \u001b[36m0.8589\u001b[0m  0.9305\n",
            "     12        \u001b[36m0.8587\u001b[0m  0.8076\n",
            "     13        \u001b[36m0.8586\u001b[0m  0.8773\n",
            "     14        \u001b[36m0.8584\u001b[0m  0.8893\n",
            "     15        \u001b[36m0.8582\u001b[0m  0.8719\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8540\u001b[0m  1.3792\n",
            "      2        \u001b[36m0.8538\u001b[0m  1.8671\n",
            "      3        \u001b[36m0.8536\u001b[0m  1.3872\n",
            "      4        \u001b[36m0.8534\u001b[0m  1.2240\n",
            "      5        \u001b[36m0.8531\u001b[0m  1.2241\n",
            "      6        \u001b[36m0.8531\u001b[0m  1.2493\n",
            "      7        \u001b[36m0.8528\u001b[0m  1.2729\n",
            "      8        \u001b[36m0.8528\u001b[0m  1.2427\n",
            "      9        \u001b[36m0.8526\u001b[0m  1.2751\n",
            "     10        \u001b[36m0.8521\u001b[0m  1.2521\n",
            "     11        \u001b[36m0.8519\u001b[0m  1.8477\n",
            "     12        \u001b[36m0.8516\u001b[0m  1.5924\n",
            "     13        \u001b[36m0.8513\u001b[0m  1.2473\n",
            "     14        \u001b[36m0.8509\u001b[0m  1.2649\n",
            "     15        \u001b[36m0.8505\u001b[0m  1.2033\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8742\u001b[0m  0.4639\n",
            "      2        \u001b[36m0.8683\u001b[0m  0.4013\n",
            "      3        \u001b[36m0.8665\u001b[0m  0.4859\n",
            "      4        \u001b[36m0.8655\u001b[0m  0.4076\n",
            "      5        \u001b[36m0.8641\u001b[0m  0.4246\n",
            "      6        \u001b[36m0.8624\u001b[0m  0.4590\n",
            "      7        \u001b[36m0.8597\u001b[0m  0.4315\n",
            "      8        \u001b[36m0.8551\u001b[0m  0.4314\n",
            "      9        \u001b[36m0.8467\u001b[0m  0.4239\n",
            "     10        \u001b[36m0.8293\u001b[0m  0.6660\n",
            "     11        \u001b[36m0.7873\u001b[0m  0.6097\n",
            "     12        \u001b[36m0.6955\u001b[0m  0.6455\n",
            "     13        0.7267  0.6607\n",
            "     14        \u001b[36m0.5059\u001b[0m  0.4514\n",
            "     15        \u001b[36m0.4130\u001b[0m  0.4548\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8612\u001b[0m  0.8173\n",
            "      2        \u001b[36m0.8591\u001b[0m  1.0893\n",
            "      3        \u001b[36m0.8579\u001b[0m  0.8915\n",
            "      4        \u001b[36m0.8564\u001b[0m  0.8514\n",
            "      5        \u001b[36m0.8538\u001b[0m  0.8362\n",
            "      6        \u001b[36m0.8483\u001b[0m  0.8796\n",
            "      7        \u001b[36m0.8350\u001b[0m  0.8454\n",
            "      8        \u001b[36m0.7904\u001b[0m  0.8525\n",
            "      9        \u001b[36m0.6643\u001b[0m  0.8948\n",
            "     10        \u001b[36m0.6489\u001b[0m  1.1299\n",
            "     11        \u001b[36m0.4756\u001b[0m  1.2696\n",
            "     12        0.5171  1.0417\n",
            "     13        \u001b[36m0.4151\u001b[0m  0.8912\n",
            "     14        0.4956  0.8384\n",
            "     15        \u001b[36m0.3992\u001b[0m  0.8723\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8536\u001b[0m  1.2049\n",
            "      2        \u001b[36m0.8529\u001b[0m  1.2325\n",
            "      3        \u001b[36m0.8523\u001b[0m  1.2402\n",
            "      4        \u001b[36m0.8508\u001b[0m  1.2298\n",
            "      5        \u001b[36m0.8484\u001b[0m  1.7038\n",
            "      6        \u001b[36m0.8409\u001b[0m  1.7781\n",
            "      7        \u001b[36m0.8127\u001b[0m  1.3374\n",
            "      8        \u001b[36m0.6774\u001b[0m  1.2665\n",
            "      9        \u001b[36m0.6062\u001b[0m  1.2337\n",
            "     10        \u001b[36m0.5551\u001b[0m  1.2481\n",
            "     11        0.6211  1.2230\n",
            "     12        \u001b[36m0.5387\u001b[0m  1.2601\n",
            "     13        \u001b[36m0.4560\u001b[0m  1.2453\n",
            "     14        0.5408  1.4916\n",
            "Stopping since train_loss has not improved in the last 2 epochs.\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8760\u001b[0m  0.4264\n",
            "      2        \u001b[36m0.8685\u001b[0m  0.4724\n",
            "      3        \u001b[36m0.8665\u001b[0m  0.4547\n",
            "      4        \u001b[36m0.8654\u001b[0m  0.4041\n",
            "      5        \u001b[36m0.8641\u001b[0m  0.5015\n",
            "      6        \u001b[36m0.8623\u001b[0m  0.4549\n",
            "      7        \u001b[36m0.8597\u001b[0m  0.4170\n",
            "      8        \u001b[36m0.8557\u001b[0m  0.5027\n",
            "      9        \u001b[36m0.8476\u001b[0m  0.4703\n",
            "     10        \u001b[36m0.8320\u001b[0m  0.4172\n",
            "     11        \u001b[36m0.7962\u001b[0m  0.4550\n",
            "     12        \u001b[36m0.7070\u001b[0m  0.4388\n",
            "     13        \u001b[36m0.5366\u001b[0m  0.4621\n",
            "     14        1.1098  0.4847\n",
            "Stopping since train_loss has not improved in the last 2 epochs.\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8597\u001b[0m  1.0427\n",
            "      2        \u001b[36m0.8586\u001b[0m  1.2554\n",
            "      3        \u001b[36m0.8571\u001b[0m  1.0919\n",
            "      4        \u001b[36m0.8548\u001b[0m  0.8751\n",
            "      5        \u001b[36m0.8500\u001b[0m  0.8898\n",
            "      6        \u001b[36m0.8380\u001b[0m  0.8560\n",
            "      7        \u001b[36m0.7992\u001b[0m  0.9059\n",
            "      8        \u001b[36m0.6647\u001b[0m  0.8045\n",
            "      9        0.7752  0.8864\n",
            "     10        \u001b[36m0.5211\u001b[0m  0.9087\n",
            "     11        \u001b[36m0.4541\u001b[0m  0.8573\n",
            "     12        \u001b[36m0.4273\u001b[0m  0.8719\n",
            "     13        0.6459  0.9290\n",
            "     14        \u001b[36m0.4057\u001b[0m  0.8943\n",
            "     15        \u001b[36m0.3763\u001b[0m  1.2678\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8540\u001b[0m  1.2374\n",
            "      2        \u001b[36m0.8533\u001b[0m  1.2775\n",
            "      3        \u001b[36m0.8529\u001b[0m  1.2219\n",
            "      4        \u001b[36m0.8505\u001b[0m  1.2812\n",
            "      5        \u001b[36m0.8473\u001b[0m  1.2643\n",
            "      6        \u001b[36m0.8359\u001b[0m  1.2151\n",
            "      7        \u001b[36m0.7923\u001b[0m  1.2599\n",
            "      8        \u001b[36m0.6991\u001b[0m  1.5202\n",
            "      9        0.7138  1.8841\n",
            "     10        \u001b[36m0.6256\u001b[0m  1.2448\n",
            "     11        \u001b[36m0.5633\u001b[0m  1.2867\n",
            "     12        \u001b[36m0.5151\u001b[0m  1.2984\n",
            "     13        0.6730  1.2541\n",
            "     14        \u001b[36m0.4758\u001b[0m  1.2615\n",
            "     15        \u001b[36m0.4684\u001b[0m  1.3067\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8700\u001b[0m  0.4217\n",
            "      2        \u001b[36m0.8658\u001b[0m  0.5773\n",
            "      3        \u001b[36m0.8632\u001b[0m  0.6346\n",
            "      4        \u001b[36m0.8573\u001b[0m  0.6271\n",
            "      5        \u001b[36m0.8407\u001b[0m  0.6413\n",
            "      6        \u001b[36m0.7673\u001b[0m  0.7116\n",
            "      7        0.9937  0.4204\n",
            "Stopping since train_loss has not improved in the last 2 epochs.\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8593\u001b[0m  0.8137\n",
            "      2        \u001b[36m0.8565\u001b[0m  0.8303\n",
            "      3        \u001b[36m0.8459\u001b[0m  0.8406\n",
            "      4        \u001b[36m0.7749\u001b[0m  0.8209\n",
            "      5        0.8501  0.8958\n",
            "      6        \u001b[36m0.5773\u001b[0m  0.8104\n",
            "      7        0.7951  0.8125\n",
            "Stopping since train_loss has not improved in the last 2 epochs.\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8536\u001b[0m  1.8800\n",
            "      2        \u001b[36m0.8520\u001b[0m  1.2645\n",
            "      3        \u001b[36m0.8475\u001b[0m  1.2783\n",
            "      4        \u001b[36m0.7988\u001b[0m  1.2729\n",
            "      5        \u001b[36m0.7739\u001b[0m  1.2314\n",
            "      6        \u001b[36m0.6966\u001b[0m  1.3543\n",
            "      7        \u001b[36m0.5655\u001b[0m  1.2931\n",
            "      8        0.6588  1.2441\n",
            "      9        \u001b[36m0.5536\u001b[0m  1.4178\n",
            "     10        \u001b[36m0.5188\u001b[0m  1.9759\n",
            "     11        0.8964  1.3387\n",
            "Stopping since train_loss has not improved in the last 2 epochs.\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8742\u001b[0m  0.4150\n",
            "      2        \u001b[36m0.8670\u001b[0m  0.4584\n",
            "      3        \u001b[36m0.8655\u001b[0m  0.4233\n",
            "      4        \u001b[36m0.8631\u001b[0m  0.4971\n",
            "      5        \u001b[36m0.8577\u001b[0m  0.4301\n",
            "      6        \u001b[36m0.8430\u001b[0m  0.4437\n",
            "      7        \u001b[36m0.7837\u001b[0m  0.4114\n",
            "      8        0.8213  0.5006\n",
            "Stopping since train_loss has not improved in the last 2 epochs.\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8600\u001b[0m  0.9067\n",
            "      2        \u001b[36m0.8587\u001b[0m  1.2427\n",
            "      3        \u001b[36m0.8564\u001b[0m  1.2269\n",
            "      4        \u001b[36m0.8491\u001b[0m  0.8469\n",
            "      5        \u001b[36m0.8051\u001b[0m  0.8554\n",
            "      6        \u001b[36m0.7142\u001b[0m  0.9243\n",
            "      7        0.8331  0.8650\n",
            "      8        \u001b[36m0.6867\u001b[0m  0.8713\n",
            "      9        \u001b[36m0.6502\u001b[0m  0.8406\n",
            "     10        \u001b[36m0.6419\u001b[0m  0.8344\n",
            "     11        \u001b[36m0.5796\u001b[0m  0.8472\n",
            "     12        \u001b[36m0.5001\u001b[0m  0.8326\n",
            "     13        0.5260  0.8215\n",
            "Stopping since train_loss has not improved in the last 2 epochs.\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8527\u001b[0m  1.6852\n",
            "      2        0.8578  1.2357\n",
            "Stopping since train_loss has not improved in the last 2 epochs.\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 3\n",
            "n_resources: 76548\n",
            "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8685\u001b[0m  1.6448\n",
            "      2        \u001b[36m0.8611\u001b[0m  1.6971\n",
            "      3        \u001b[36m0.8191\u001b[0m  1.6731\n",
            "      4        \u001b[36m0.6009\u001b[0m  2.4519\n",
            "      5        \u001b[36m0.4677\u001b[0m  1.7824\n",
            "      6        \u001b[36m0.3308\u001b[0m  1.6703\n",
            "      7        \u001b[36m0.3091\u001b[0m  1.7201\n",
            "      8        \u001b[36m0.2953\u001b[0m  1.9511\n",
            "      9        \u001b[36m0.2914\u001b[0m  1.6793\n",
            "     10        \u001b[36m0.2628\u001b[0m  1.8044\n",
            "     11        0.2649  2.4731\n",
            "     12        \u001b[36m0.2541\u001b[0m  1.7710\n",
            "     13        \u001b[36m0.2431\u001b[0m  1.8028\n",
            "     14        0.2557  1.6736\n",
            "Stopping since train_loss has not improved in the last 2 epochs.\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8603\u001b[0m  3.7986\n",
            "      2        \u001b[36m0.8401\u001b[0m  3.2812\n",
            "      3        \u001b[36m0.7313\u001b[0m  3.2691\n",
            "      4        \u001b[36m0.4818\u001b[0m  4.2623\n",
            "      5        \u001b[36m0.4588\u001b[0m  3.2654\n",
            "      6        \u001b[36m0.4030\u001b[0m  3.2932\n",
            "      7        \u001b[36m0.3737\u001b[0m  3.3448\n",
            "      8        \u001b[36m0.3545\u001b[0m  4.2383\n",
            "      9        \u001b[36m0.3309\u001b[0m  3.3332\n",
            "     10        \u001b[36m0.3186\u001b[0m  3.2989\n",
            "     11        0.3299  4.1513\n",
            "     12        \u001b[36m0.3068\u001b[0m  3.2768\n",
            "     13        0.3125  3.2967\n",
            "     14        \u001b[36m0.2847\u001b[0m  3.2540\n",
            "     15        0.2980  4.1602\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8524\u001b[0m  5.8993\n",
            "      2        \u001b[36m0.7203\u001b[0m  5.7536\n",
            "      3        \u001b[36m0.5468\u001b[0m  7.2777\n",
            "      4        \u001b[36m0.4477\u001b[0m  4.8120\n",
            "      5        \u001b[36m0.4090\u001b[0m  5.2009\n",
            "      6        \u001b[36m0.3994\u001b[0m  5.6475\n",
            "      7        \u001b[36m0.3874\u001b[0m  6.2014\n",
            "      8        \u001b[36m0.3787\u001b[0m  6.3447\n",
            "      9        \u001b[36m0.3704\u001b[0m  4.8146\n",
            "     10        \u001b[36m0.3622\u001b[0m  5.8585\n",
            "     11        \u001b[36m0.3550\u001b[0m  5.0040\n",
            "     12        \u001b[36m0.3474\u001b[0m  5.2342\n",
            "     13        0.3601  5.4329\n",
            "     14        \u001b[36m0.3432\u001b[0m  4.8465\n",
            "     15        0.3474  5.7566\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.6388\u001b[0m  1.9358\n",
            "      2        0.6433  2.2982\n",
            "      3        \u001b[36m0.3206\u001b[0m  1.6599\n",
            "      4        \u001b[36m0.2951\u001b[0m  1.7780\n",
            "      5        \u001b[36m0.2899\u001b[0m  1.7370\n",
            "      6        \u001b[36m0.2586\u001b[0m  1.6968\n",
            "      7        0.2767  1.6729\n",
            "      8        \u001b[36m0.2464\u001b[0m  2.0987\n",
            "      9        0.2491  2.2127\n",
            "Stopping since train_loss has not improved in the last 2 epochs.\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.6422\u001b[0m  3.3204\n",
            "      2        \u001b[36m0.4161\u001b[0m  4.2914\n",
            "      3        \u001b[36m0.3717\u001b[0m  3.3775\n",
            "      4        \u001b[36m0.3528\u001b[0m  3.3593\n",
            "      5        \u001b[36m0.3402\u001b[0m  4.0201\n",
            "      6        \u001b[36m0.3246\u001b[0m  3.4553\n",
            "      7        \u001b[36m0.3186\u001b[0m  3.2864\n",
            "      8        \u001b[36m0.3149\u001b[0m  3.2596\n",
            "      9        \u001b[36m0.2971\u001b[0m  4.1677\n",
            "     10        0.2972  3.3784\n",
            "     11        \u001b[36m0.2955\u001b[0m  3.4055\n",
            "     12        \u001b[36m0.2893\u001b[0m  3.9882\n",
            "     13        \u001b[36m0.2856\u001b[0m  3.8815\n",
            "     14        \u001b[36m0.2814\u001b[0m  3.4219\n",
            "     15        \u001b[36m0.2719\u001b[0m  3.3097\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.6544\u001b[0m  5.0523\n",
            "      2        \u001b[36m0.4674\u001b[0m  6.1705\n",
            "      3        \u001b[36m0.4219\u001b[0m  5.0181\n",
            "      4        \u001b[36m0.4056\u001b[0m  5.8202\n",
            "      5        \u001b[36m0.3825\u001b[0m  4.9421\n",
            "      6        0.3878  5.2475\n",
            "      7        \u001b[36m0.3697\u001b[0m  5.5662\n",
            "      8        \u001b[36m0.3558\u001b[0m  4.9532\n",
            "      9        0.3583  5.8310\n",
            "     10        \u001b[36m0.3512\u001b[0m  4.9064\n",
            "     11        \u001b[36m0.3428\u001b[0m  5.8044\n",
            "     12        0.3440  4.9167\n",
            "Stopping since train_loss has not improved in the last 2 epochs.\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.7471\u001b[0m  1.6266\n",
            "      2        \u001b[36m0.3875\u001b[0m  1.6665\n",
            "      3        \u001b[36m0.3593\u001b[0m  1.7822\n",
            "      4        \u001b[36m0.2923\u001b[0m  2.5999\n",
            "      5        0.3061  1.6389\n",
            "      6        \u001b[36m0.2663\u001b[0m  1.6434\n",
            "      7        0.2739  1.6961\n",
            "      8        \u001b[36m0.2469\u001b[0m  1.6065\n",
            "      9        \u001b[36m0.2428\u001b[0m  1.6693\n",
            "     10        0.2695  1.7547\n",
            "     11        \u001b[36m0.2317\u001b[0m  2.4739\n",
            "     12        0.2460  1.7048\n",
            "     13        \u001b[36m0.2311\u001b[0m  1.6998\n",
            "     14        \u001b[36m0.2271\u001b[0m  1.6977\n",
            "     15        \u001b[36m0.2225\u001b[0m  1.7211\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.7312\u001b[0m  3.8234\n",
            "      2        \u001b[36m0.4210\u001b[0m  3.3258\n",
            "      3        \u001b[36m0.3655\u001b[0m  3.5694\n",
            "      4        \u001b[36m0.3599\u001b[0m  4.1994\n",
            "      5        \u001b[36m0.3331\u001b[0m  3.3653\n",
            "      6        \u001b[36m0.3232\u001b[0m  3.3744\n",
            "      7        \u001b[36m0.3216\u001b[0m  3.6695\n",
            "      8        \u001b[36m0.3132\u001b[0m  3.8684\n",
            "      9        \u001b[36m0.2924\u001b[0m  3.3922\n",
            "     10        0.3101  3.3638\n",
            "Stopping since train_loss has not improved in the last 2 epochs.\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.6450\u001b[0m  5.9251\n",
            "      2        \u001b[36m0.4769\u001b[0m  5.0686\n",
            "      3        \u001b[36m0.4237\u001b[0m  5.0059\n",
            "      4        \u001b[36m0.4120\u001b[0m  5.9852\n",
            "      5        \u001b[36m0.3956\u001b[0m  5.0199\n",
            "      6        \u001b[36m0.3745\u001b[0m  5.8246\n",
            "      7        \u001b[36m0.3697\u001b[0m  4.9489\n",
            "      8        \u001b[36m0.3666\u001b[0m  5.4559\n",
            "      9        \u001b[36m0.3589\u001b[0m  5.2531\n",
            "     10        \u001b[36m0.3520\u001b[0m  4.8963\n",
            "     11        0.3586  5.9948\n",
            "     12        \u001b[36m0.3477\u001b[0m  4.9626\n",
            "     13        \u001b[36m0.3416\u001b[0m  5.8818\n",
            "     14        0.3431  4.9322\n",
            "     15        \u001b[36m0.3405\u001b[0m  5.2855\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.6100\u001b[0m  9.1058\n",
            "      2        \u001b[36m0.4661\u001b[0m  8.7101\n",
            "      3        \u001b[36m0.4400\u001b[0m  8.3936\n",
            "      4        \u001b[36m0.4199\u001b[0m  9.0258\n",
            "      5        \u001b[36m0.4089\u001b[0m  9.2628\n",
            "      6        \u001b[36m0.4036\u001b[0m  8.2036\n",
            "      7        \u001b[36m0.3917\u001b[0m  9.2133\n",
            "      8        \u001b[36m0.3883\u001b[0m  9.2842\n",
            "      9        \u001b[36m0.3803\u001b[0m  8.4739\n",
            "     10        \u001b[36m0.3725\u001b[0m  8.8243\n",
            "     11        0.3755  9.3083\n",
            "     12        \u001b[36m0.3673\u001b[0m  9.0023\n",
            "     13        0.3691  8.1232\n",
            "     14        \u001b[36m0.3609\u001b[0m  9.0878\n",
            "     15        0.3615  9.0000\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.6419\u001b[0m  6.0890\n",
            "      2        \u001b[36m0.4524\u001b[0m  6.9818\n",
            "      3        \u001b[36m0.4098\u001b[0m  6.2509\n",
            "      4        \u001b[36m0.3937\u001b[0m  7.0173\n",
            "      5        \u001b[36m0.3780\u001b[0m  6.0398\n",
            "      6        \u001b[36m0.3654\u001b[0m  6.9999\n",
            "      7        \u001b[36m0.3599\u001b[0m  6.0951\n",
            "      8        \u001b[36m0.3506\u001b[0m  7.0814\n",
            "      9        \u001b[36m0.3451\u001b[0m  6.1506\n",
            "     10        0.3458  7.0129\n",
            "     11        \u001b[36m0.3382\u001b[0m  6.2105\n",
            "     12        \u001b[36m0.3372\u001b[0m  7.0124\n",
            "     13        \u001b[36m0.3292\u001b[0m  6.7603\n",
            "     14        \u001b[36m0.3290\u001b[0m  6.4874\n",
            "     15        \u001b[36m0.3238\u001b[0m  6.8027\n",
            "τ* (last-fold, max F1): 0.4045\n",
            "\n",
            "=== MLP (skorch, SGD) — Halving v2 ===\n",
            "best params: {'clf__batch_size': 2048, 'clf__module__hidden': (1024, 128), 'clf__optimizer__lr': 0.5, 'clf__optimizer__weight_decay': 0.0001}\n",
            "param count: 215297 | epochs: 15\n",
            "best CV ROC-AUC: 0.8466\n",
            "train ROC-AUC: 0.9508\n",
            "test  ROC-AUC: 0.8488\n",
            "train acc @τ*: 0.8739\n",
            "test  acc @τ*: 0.771\n",
            "train AP: 0.9326\n",
            "test  AP: 0.7441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, hidden=[1024, 128], out_dim=1, dropout_p=0.0):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        dims = [in_dim] + hidden\n",
        "        for i in range(len(dims)-1):\n",
        "            layers += [nn.Linear(dims[i], dims[i+1]), nn.ReLU()]\n",
        "            if dropout_p > 0: layers += [nn.Dropout(p=dropout_p)]\n",
        "        layers += [nn.Linear(hidden[-1] if hidden else in_dim, out_dim)]  # out_dim=1 for binary\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(-1)  # logits; don’t add Sigmoid here\n"
      ],
      "metadata": {
        "id": "dEXl1_BQIs2n"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_layers(model):\n",
        "    return [m for m in model.modules() if isinstance(m, nn.Linear)]\n",
        "\n",
        "def freeze_all_but_last_k(model, k=2):\n",
        "    layers = linear_layers(model)\n",
        "    # Freeze all first:\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "    # Unfreeze last k Linear layers:\n",
        "    for m in layers[-k:]:\n",
        "        for p in m.parameters():\n",
        "            p.requires_grad = True\n",
        "    # Report counts (needed for RO cap)\n",
        "    tot = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Params total={tot:,} | trainable(last {k})={trainable:,}\")\n",
        "    return trainable\n",
        "\n",
        "# Example: freeze all but last 1–3 layers for Part 1 RO\n",
        "model = MLP(in_dim=Xtr_t.shape[1], hidden=[1024, 128], out_dim=1).to(device)\n",
        "trainable = freeze_all_but_last_k(model, k=1)\n",
        "assert trainable <= 50_000, \"RO parameter cap exceeded.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3-VOP_qNlk2",
        "outputId": "164bc645-2056-4cef-b7ea-a1f6b0449dfb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params total=215,297 | trainable(last 1)=129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"classification\"  # or \"regression\"\n",
        "criterion = nn.BCEWithLogitsLoss(reduction='mean')\n"
      ],
      "metadata": {
        "id": "PS0LkF73OXB8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, roc_curve,\n",
        "    average_precision_score, precision_recall_curve,\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "# --- Fit isotonic calibrator on VALIDATION (once) ---\n",
        "model.eval()\n",
        "val_logits, val_y = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in val_loader:\n",
        "        val_logits.append(model(xb).cpu().numpy())\n",
        "        val_y.append(yb.cpu().numpy())\n",
        "import numpy as np\n",
        "val_logits = np.concatenate(val_logits)\n",
        "val_y      = np.concatenate(val_y).astype(int)\n",
        "\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "def sigmoid_np(x):  # keep your helper available early\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "val_p = sigmoid_np(val_logits)                   # probs from logits\n",
        "iso   = IsotonicRegression(out_of_bounds=\"clip\")\n",
        "iso.fit(val_p, val_y)                            # <-- trains the calibrator\n",
        "\n",
        "def calibrate_probs_isotonic(p):                 # p are uncalibrated probs\n",
        "    return iso.transform(p)\n",
        "\n",
        "def sigmoid_np(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "def pick_threshold_max_f1(y_true, scores):\n",
        "    P, R, T = precision_recall_curve(y_true, scores)  # len(T) = len(P)-1\n",
        "    f1 = 2 * P[1:] * R[1:] / (P[1:] + R[1:] + 1e-12)\n",
        "    i = int(np.nanargmax(f1))\n",
        "    return float(T[i]), float(f1[i]), float(P[i+1]), float(R[i+1])\n",
        "\n",
        "def eval_on_loader(model, loader, threshold=None, pick_rule=\"max_f1\"):\n",
        "    \"\"\"\n",
        "    model: returns logits (no sigmoid).\n",
        "    loader: DataLoader yielding (X, y) with y in {0,1}.\n",
        "    threshold:\n",
        "        - None + pick_rule=\"max_f1\" -> choose t that maximizes F1 on these scores.\n",
        "        - float in [0,1] -> use exactly this probability cutoff.\n",
        "    Returns: dict of metrics + curves + chosen threshold.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    logits, ys = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            logits.append(model(xb).cpu().numpy())\n",
        "            ys.append(yb.cpu().numpy())\n",
        "    logits = np.concatenate(logits).astype(np.float64)\n",
        "    y = np.concatenate(ys).astype(np.int32)\n",
        "\n",
        "    # Probabilities for metrics that need them\n",
        "    p = sigmoid_np(logits)\n",
        "    p = calibrate_probs_isotonic(p)\n",
        "\n",
        "    # Curves + areas (threshold-free summaries)\n",
        "    auroc = roc_auc_score(y, logits)                 # OK to use logits\n",
        "    fpr, tpr, roc_th = roc_curve(y, logits)\n",
        "    aupr  = average_precision_score(y, p)            # area under PR; aka AUC-PR\n",
        "    prec_curve, rec_curve, pr_th = precision_recall_curve(y, p)\n",
        "\n",
        "    # Pick threshold if not given\n",
        "    if threshold is None:\n",
        "        if pick_rule == \"max_f1\":\n",
        "            t_star, _, _, _ = pick_threshold_max_f1(y, p)\n",
        "        elif pick_rule == \"fixed_0p5\":\n",
        "            t_star = 0.5\n",
        "        elif pick_rule == \"match_prevalence\":\n",
        "            # choose t so predicted positive rate roughly matches prevalence\n",
        "            prev = y.mean()\n",
        "            # find t where mean(p >= t) is closest to prev\n",
        "            cand = np.linspace(0.0, 1.0, 1001)\n",
        "            idx = np.argmin(np.abs((p[:,None] >= cand).mean(axis=0) - prev))\n",
        "            t_star = float(cand[idx])\n",
        "        else:\n",
        "            t_star = 0.5\n",
        "    else:\n",
        "        t_star = float(threshold)\n",
        "\n",
        "    yhat = (p >= t_star).astype(int)\n",
        "\n",
        "    # At-threshold metrics (“calibrated”)\n",
        "    acc  = accuracy_score(y, yhat)\n",
        "    prec = precision_score(y, yhat, zero_division=0)\n",
        "    rec  = recall_score(y, yhat, zero_division=0)\n",
        "    f1   = f1_score(y, yhat, zero_division=0)\n",
        "    prev = float(y.mean())\n",
        "    tn, fp, fn, tp = confusion_matrix(y, yhat).ravel()\n",
        "\n",
        "    return {\n",
        "        \"threshold\": t_star,\n",
        "        # single-number summaries from curves:\n",
        "        \"auroc\": float(auroc),          # area under ROC curve\n",
        "        \"aupr\": float(aupr),            # area under Precision-Recall (AUC-PR)\n",
        "        # at-threshold (“calibrated”) scores:\n",
        "        \"accuracy\": float(acc),\n",
        "        \"precision\": float(prec),\n",
        "        \"recall\": float(rec),\n",
        "        \"f1\": float(f1),\n",
        "        \"prevalence\": prev,\n",
        "        \"confusion\": {\"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp)},\n",
        "        # full curves if you want to plot later:\n",
        "        \"roc_curve\": {\"fpr\": fpr, \"tpr\": tpr, \"thr\": roc_th},\n",
        "        \"pr_curve\":  {\"precision\": prec_curve, \"recall\": rec_curve, \"thr\": pr_th},\n",
        "    }\n"
      ],
      "metadata": {
        "id": "fymIoiByO-GD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: choose threshold that maximizes F1 on the validation split\n",
        "val_metrics = eval_on_loader(model, val_loader, threshold=None, pick_rule=\"max_f1\")\n",
        "print(val_metrics[\"threshold\"], val_metrics[\"auroc\"], val_metrics[\"aupr\"],\n",
        "      val_metrics[\"precision\"], val_metrics[\"recall\"], val_metrics[\"f1\"])\n",
        "t_star = val_metrics[\"threshold\"]\n",
        "test_metrics = eval_on_loader(model, test_loader, threshold=t_star)\n",
        "def pretty_print_metrics(name, M):\n",
        "    C = M[\"confusion\"]\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(f\"Threshold (picked): {M['threshold']:.3f}\")\n",
        "    print(f\"AUROC: {M['auroc']:.3f}  |  AUPR: {M['aupr']:.3f}  |  Prevalence: {M['prevalence']:.3f}\")\n",
        "    print(f\"Accuracy:  {M.get('accuracy', float('nan')):.3f}\")\n",
        "    print(f\"Precision: {M['precision']:.3f}  |  Recall: {M['recall']:.3f}  |  F1: {M['f1']:.3f}\")\n",
        "    print(f\"Confusion @ threshold → TN:{C['tn']}  FP:{C['fp']}  FN:{C['fn']}  TP:{C['tp']}\")\n",
        "\n",
        "# usage\n",
        "pretty_print_metrics(\"Validation\", val_metrics)\n",
        "pretty_print_metrics(\"Test\", test_metrics)\n"
      ],
      "metadata": {
        "id": "L51R2d4ZqJXo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00215f7a-7a2f-43f0-e791-688ad9116161"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.29145434498786926 0.6763812295568926 0.5576533152432483 0.49799156457119903 0.6846610520502554 0.5765943840474391\n",
            "\n",
            "=== Validation ===\n",
            "Threshold (picked): 0.291\n",
            "AUROC: 0.676  |  AUPR: 0.558  |  Prevalence: 0.379\n",
            "Accuracy:  0.619\n",
            "Precision: 0.498  |  Recall: 0.685  |  F1: 0.577\n",
            "Confusion @ threshold → TN:6866  FP:4999  FN:2284  TP:4959\n",
            "\n",
            "=== Test ===\n",
            "Threshold (picked): 0.291\n",
            "AUROC: 0.628  |  AUPR: 0.410  |  Prevalence: 0.315\n",
            "Accuracy:  0.486\n",
            "Precision: 0.359  |  Recall: 0.808  |  F1: 0.498\n",
            "Confusion @ threshold → TN:5489  FP:10749  FN:1432  TP:6033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, numpy as np\n",
        "from torch import nn\n",
        "\n",
        "def get_last_k_linear_layers(model: nn.Module, k: int):\n",
        "    layers = [m for m in model.modules() if isinstance(m, nn.Linear)]\n",
        "    return layers[-k:] if k > 0 else []\n",
        "\n",
        "@torch.no_grad()\n",
        "def tail_to_vector(model: nn.Module, k: int) -> torch.Tensor:\n",
        "    \"\"\"Flatten last-k Linear layers' (weight, bias) into one 1-D tensor on CPU.\"\"\"\n",
        "    vecs = []\n",
        "    for layer in get_last_k_linear_layers(model, k):\n",
        "        vecs += [layer.weight.detach().flatten().cpu()]\n",
        "        if layer.bias is not None:\n",
        "            vecs += [layer.bias.detach().flatten().cpu()]\n",
        "    return torch.cat(vecs) if vecs else torch.empty(0)\n",
        "\n",
        "@torch.no_grad()\n",
        "def vector_to_tail(model: nn.Module, k: int, vec: torch.Tensor, device=None):\n",
        "    \"\"\"Write a flat vector back into the last-k Linear layers in order.\"\"\"\n",
        "    if device is None: device = next(model.parameters()).device\n",
        "    idx = 0\n",
        "    for layer in get_last_k_linear_layers(model, k):\n",
        "        W = layer.weight\n",
        "        nW = W.numel()\n",
        "        W.copy_(vec[idx:idx+nW].view_as(W).to(device))\n",
        "        idx += nW\n",
        "        if layer.bias is not None:\n",
        "            b = layer.bias\n",
        "            nb = b.numel()\n",
        "            b.copy_(vec[idx:idx+nb].view_as(b).to(device))\n",
        "            idx += nb\n",
        "    assert idx == vec.numel(), \"Vector length mismatch when restoring tail params.\"\n"
      ],
      "metadata": {
        "id": "rzP6DjHV4ISu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "\n",
        "def run_epoch(model, loader, optimizer=None):\n",
        "    is_train = optimizer is not None\n",
        "    model.train(is_train)\n",
        "    total_loss, n, grad_evals = 0.0, 0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        if is_train:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.set_grad_enabled(is_train):\n",
        "            pred = model(xb)\n",
        "            loss = criterion(pred, yb)\n",
        "            if is_train:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                grad_evals += 1  # count one optimizer step = one gradient evaluation\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "        n += xb.size(0)\n",
        "    return total_loss / n, grad_evals\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total, n_batches = 0.0, 0\n",
        "    for xb, yb in loader:\n",
        "        total += criterion(model(xb.to(device)), yb.to(device)).item()\n",
        "        n_batches += 1\n",
        "    return total / n_batches"
      ],
      "metadata": {
        "id": "mbkLdtvTTi-7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# objective wrapper that takes a candidate param vector\n",
        "@torch.no_grad()\n",
        "def objective_from_vec(model, k, vec, val_loader):\n",
        "    vector_to_tail(model, k, vec)\n",
        "    return evaluate(model, val_loader)  # returns mean val loss (scalar float)\n",
        "\n",
        "def objective_from_vec_safe(model, k, vec, val_loader, state):\n",
        "    loss = objective_from_vec(model, k, vec, val_loader)  # calls evaluate() -> model.eval()\n",
        "    state[\"evals\"] += 1\n",
        "    if not np.isfinite(loss):\n",
        "        state[\"failures\"] += 1\n",
        "        return 1e9  # count as an eval; reject candidate\n",
        "    return loss\n",
        "\n"
      ],
      "metadata": {
        "id": "ceF_BqKGS3cU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eRv0Bx_jLVV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rhc(model, k, val_loader, start_vec=None, sigma=0.01, restarts=2, budget=2000, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    state = {\"evals\": 0, \"failures\": 0}\n",
        "\n",
        "    best_vec = start_vec.clone() if start_vec is not None else tail_to_vector(model, k)\n",
        "    best_loss = objective_from_vec_safe(model, k, best_vec, val_loader, state)  # evals += 1 inside\n",
        "    history = [(state[\"evals\"], best_loss)]\n",
        "\n",
        "    for r in range(restarts + 1):\n",
        "        cur_vec, cur_loss = best_vec.clone(), best_loss\n",
        "\n",
        "        while state[\"evals\"] < budget:\n",
        "            step = torch.from_numpy(rng.normal(0.0, sigma, size=cur_vec.numel()).astype(np.float32))\n",
        "            cand = cur_vec + step\n",
        "            loss = objective_from_vec_safe(model, k, cand, val_loader, state)  # evals += 1 inside\n",
        "\n",
        "            if loss < cur_loss:\n",
        "                cur_vec, cur_loss = cand, loss\n",
        "                if loss < best_loss:\n",
        "                    best_vec, best_loss = cand.clone(), loss\n",
        "\n",
        "            history.append((state[\"evals\"], best_loss))\n",
        "\n",
        "        # simple restart around global best (counts as an eval too)\n",
        "        if r < restarts:\n",
        "            cur_vec = best_vec + torch.from_numpy(rng.normal(0.0, sigma*2, size=best_vec.numel()).astype(np.float32))\n",
        "            cur_loss = objective_from_vec_safe(model, k, cur_vec, val_loader, state)\n",
        "            history.append((state[\"evals\"], min(best_loss, cur_loss)))\n",
        "\n",
        "    vector_to_tail(model, k, best_vec)\n",
        "    return {\n",
        "        \"best_loss\": best_loss,\n",
        "        \"best_vec\": best_vec,\n",
        "        \"history\": history,            # [(eval_count, best_so_far_loss), ...]\n",
        "        \"evals\": state[\"evals\"],       # total function evaluations\n",
        "        \"failures\": state[\"failures\"], # NaN/Inf candidates counted as evals\n",
        "    }\n"
      ],
      "metadata": {
        "id": "oj16vSiqS3lc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sa(model, k, val_loader, start_vec=None, sigma=0.01, T0=1.0, decay=0.995, budget=2000, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    state = {\"evals\": 0, \"failures\": 0}\n",
        "\n",
        "    cur = start_vec.clone() if start_vec is not None else tail_to_vector(model, k)\n",
        "    cur_loss = objective_from_vec_safe(model, k, cur, val_loader, state)  # counts 1 eval\n",
        "    best, best_loss = cur.clone(), cur_loss\n",
        "    T = T0\n",
        "    history = [(state[\"evals\"], best_loss)]\n",
        "\n",
        "    while state[\"evals\"] < budget:\n",
        "        step = torch.from_numpy(rng.normal(0.0, sigma, size=cur.numel()).astype(np.float32))\n",
        "        cand = cur + step\n",
        "        loss = objective_from_vec_safe(model, k, cand, val_loader, state)  # increments evals\n",
        "\n",
        "        d = loss - cur_loss\n",
        "        # accept if better, or probabilistically if worse\n",
        "        if d < 0 or rng.random() < np.exp(-d / max(T, 1e-8)):\n",
        "            cur, cur_loss = cand, loss\n",
        "            if loss < best_loss:\n",
        "                best, best_loss = cand.clone(), loss\n",
        "\n",
        "        history.append((state[\"evals\"], best_loss))\n",
        "        T *= decay  # cool down\n",
        "\n",
        "    vector_to_tail(model, k, best)\n",
        "    return {\n",
        "        \"best_loss\": best_loss,\n",
        "        \"best_vec\":  best,\n",
        "        \"history\":   history,\n",
        "        \"evals\":     state[\"evals\"],       # total function evaluations\n",
        "        \"failures\":  state[\"failures\"],    # NaN/Inf candidates counted as evals\n",
        "    }\n"
      ],
      "metadata": {
        "id": "BczMVUEhS3vr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ga(model, k, val_loader, start_vec=None, pop_size=32, elite_frac=0.1, mut_rate=0.1, sigma=0.01,\n",
        "       generations=200, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    state = {\"evals\": 0, \"failures\": 0}\n",
        "\n",
        "    base_vec = (start_vec.clone() if start_vec is not None else tail_to_vector(model, k))\n",
        "    base = base_vec.numpy().astype(np.float32)\n",
        "    dim = base.size\n",
        "    elite_k = max(1, int(pop_size * elite_frac))\n",
        "\n",
        "    # init population around base\n",
        "    pop = base[None, :] + rng.normal(0, sigma, size=(pop_size, dim)).astype(np.float32)\n",
        "\n",
        "    def fitness(vec_np):\n",
        "        vec_t = torch.from_numpy(vec_np)\n",
        "        return objective_from_vec_safe(model, k, vec_t, val_loader, state)  # increments state[\"evals\"]\n",
        "\n",
        "    # evaluate initial population\n",
        "    losses = np.array([fitness(v) for v in pop], dtype=np.float32)\n",
        "    best_idx = int(np.argmin(losses))\n",
        "    best = pop[best_idx].copy()\n",
        "    best_loss = float(losses[best_idx])\n",
        "    history = [(state[\"evals\"], best_loss)]\n",
        "\n",
        "    for _ in range(generations):\n",
        "        # select by rank (lower loss is better)\n",
        "        idx = np.argsort(losses)\n",
        "        elites  = pop[idx[:elite_k]]\n",
        "        parents = pop[idx[:pop_size // 2]]\n",
        "\n",
        "        # uniform crossover + mutation\n",
        "        children = []\n",
        "        while len(children) < pop_size - elite_k:\n",
        "            a = parents[rng.integers(0, parents.shape[0])]\n",
        "            b = parents[rng.integers(0, parents.shape[0])]\n",
        "            mask = rng.random(dim) < 0.5\n",
        "            child = np.where(mask, a, b).astype(np.float32)\n",
        "            # mutation\n",
        "            mut_mask = rng.random(dim) < mut_rate\n",
        "            if mut_mask.any():\n",
        "                child[mut_mask] += rng.normal(0, sigma, size=mut_mask.sum()).astype(np.float32)\n",
        "            children.append(child)\n",
        "\n",
        "        pop = np.vstack([elites, np.stack(children, axis=0)])\n",
        "\n",
        "        # evaluate new population\n",
        "        losses = np.array([fitness(v) for v in pop], dtype=np.float32)\n",
        "        gen_best_idx = int(np.argmin(losses))\n",
        "        gen_best_loss = float(losses[gen_best_idx])\n",
        "        if gen_best_loss < best_loss:\n",
        "            best_loss = gen_best_loss\n",
        "            best = pop[gen_best_idx].copy()\n",
        "\n",
        "        history.append((state[\"evals\"], best_loss))\n",
        "\n",
        "    # restore best weights\n",
        "    vector_to_tail(model, k, torch.from_numpy(best))\n",
        "    return {\n",
        "        \"best_loss\": best_loss,\n",
        "        \"best_vec\":  torch.from_numpy(best),\n",
        "        \"history\":   history,            # [(eval_count, best_so_far_loss), ...]\n",
        "        \"evals\":     state[\"evals\"],     # total function evaluations\n",
        "        \"failures\":  state[\"failures\"],  # NaN/Inf candidates counted as evals\n",
        "    }\n"
      ],
      "metadata": {
        "id": "AVSQGPQtTCaU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline = evaluate(model, val_loader)\n",
        "print(\"baseline val loss:\", baseline)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SisxjNWTcUO-",
        "outputId": "bdb1f4d1-c459-4a03-ea61-c2db966697ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "baseline val loss: 0.5442442046968561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = next(iter(val_loader))\n",
        "logits = model(xb.to(device))        # expect shape [N]  (not [N,1])\n",
        "print(\"logits shape:\", tuple(logits.shape), \"yb shape:\", tuple(yb.shape))\n",
        "\n",
        "# If logits is [N,1], either:\n",
        "#   (a) return logits.squeeze(-1) in forward(), or\n",
        "#   (b) use yb = yb.unsqueeze(1).float() in the loss call.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTiSxX7UdV8t",
        "outputId": "d3e8cdaf-0ab0-4295-8a5c-3635f2c041a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits shape: (1024,) yb shape: (1024,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "ybf = yb.to(device).float()\n",
        "l_mean = F.binary_cross_entropy_with_logits(logits, ybf, reduction='mean')\n",
        "l_sum  = F.binary_cross_entropy_with_logits(logits, ybf, reduction='sum')\n",
        "print(\"one-batch BCE mean:\", float(l_mean), \" sum:\", float(l_sum))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siEdGY41djmj",
        "outputId": "3ba25efa-1628-4575-f62f-233f68837d29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one-batch BCE mean: 0.5316610932350159  sum: 544.4209594726562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(criterion), getattr(criterion, \"reduction\", None))\n",
        "# Should be: <class 'torch.nn.modules.loss.BCEWithLogitsLoss'>  'mean'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMT-px3WdQsc",
        "outputId": "e58a36a9-dd47-4fa6-a267-0f5c1206d7d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.nn.modules.loss.BCEWithLogitsLoss'> mean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure: model has all but last k layers frozen, criterion is BCEWithLogitsLoss,\n",
        "# val_loader is fixed, model.eval() is used inside evaluate() (it is).\n",
        "\n",
        "k = 1 # how many frozen layers there are\n",
        "start_vec = tail_to_vector(model, k)  # warm start from your trained checkpoint\n",
        "\n",
        "# RHC\n",
        "rhc_res = rhc(model, k, val_loader, start_vec=start_vec, sigma=0.01, restarts=2, budget=2000, seed=42)\n",
        "print(\"RHC best val loss:\", rhc_res[\"best_loss\"])\n",
        "print(f\"RHC failure rate: {rhc_res['failures']/rhc_res['evals']:.2%}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpKkMWyGTCy7",
        "outputId": "aa228726-c780-470a-fdcd-605fcf364bf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RHC best val loss: 0.47773997877773483\n",
            "RHC failure rate: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SA\n",
        "sa_res  = sa(model, k, val_loader, start_vec=start_vec, sigma=0.01, T0=1.0, decay=0.997, budget=2000, seed=42)\n",
        "print(\"SA best val loss:\", sa_res[\"best_loss\"])\n",
        "print(f\"SA failure rate: {sa_res['failures']/sa_res['evals']:.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtNpGFVlNfcV",
        "outputId": "a7b661e5-e265-465e-b025-823976d293fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SA best val loss: 0.5409733323674453\n",
            "SA failure rate: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GA\n",
        "ga_res  = ga(model, k, val_loader, start_vec, pop_size=32, elite_frac=0.125, mut_rate=0.05, sigma=0.02,\n",
        "             generations=(2000//32), seed=42) # 2000/32 to mathc compute budget with other RO\n",
        "print(\"GA best val loss:\", ga_res[\"best_loss\"])\n",
        "print(f\"GA failure rate: {ga_res['failures']/ga_res['evals']:.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBTQAohPFYQb",
        "outputId": "5c37644e-87b3-4dae-cac4-a82540b56ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GA best val loss: 0.5301499962806702\n",
            "GA failure rate: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same starting vector sanity check\n",
        "orig_tail = start_vec.clone()\n",
        "\n",
        "vector_to_tail(model, k, orig_tail)\n",
        "print(\"Matches start?\", torch.allclose(tail_to_vector(model, k), orig_tail))  # -> True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uY-xz8gqAWd7",
        "outputId": "b2a1915d-a870-43a5-fbd1-8caffd4ca7d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matches start? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QDA3-Ah4Ke2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time, numpy as np, torch\n",
        "\n",
        "def safe_start_vec(model, k, start_vec=None, noise_sigma=0.0, seed=0, clamp=None):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    base = (start_vec.clone() if start_vec is not None else tail_to_vector(model, k)).detach()\n",
        "    if noise_sigma > 0:\n",
        "        base = base + torch.from_numpy(rng.normal(0, noise_sigma, size=base.numel()).astype(np.float32)).view_as(base)\n",
        "    if clamp is not None:\n",
        "        lo, hi = clamp\n",
        "        base = torch.clamp(base, lo, hi)\n",
        "    return base.contiguous()\n",
        "\n",
        "def micro_tune(run_once, configs, hard_eval_cap=500, hard_time_s=3.0, seed=0):\n",
        "    \"\"\"\n",
        "    run_once(cfg, eval_cap, time_s, seed) -> dict(score=..., evals=..., picked_cfg=cfg)\n",
        "    score: the *higher is better* metric your objective returns (neg loss / accuracy).\n",
        "    \"\"\"\n",
        "    best = None\n",
        "    start_t = time.time()\n",
        "    for cfg in configs:\n",
        "        # Give each config a tiny, equal budget (both eval + time)\n",
        "        per_cfg_time = max(0.5, (hard_time_s - (time.time() - start_t)) / max(1, (len(configs))))\n",
        "        if per_cfg_time <= 0: break\n",
        "        out = run_once(cfg, eval_cap=hard_eval_cap // max(1,len(configs)), time_s=per_cfg_time, seed=seed)\n",
        "        if out is None: continue\n",
        "        score = out[\"score\"]\n",
        "        # Prefer higher score; tie-break by fewer evals\n",
        "        if (best is None) or (score > best[\"score\"]) or (score == best[\"score\"] and out[\"evals\"] < best[\"evals\"]):\n",
        "            best = {**out, \"picked_cfg\": cfg}\n",
        "    return best\n"
      ],
      "metadata": {
        "id": "pix_MOhjqssd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ga_quick_tune(model, k, val_loader, start_vec=None, seed=0):\n",
        "    # tiny grid (2×2 = 4 runs)\n",
        "    grid = [\n",
        "        {\"mut_rate\": 0.05, \"sigma\": 0.005},\n",
        "        {\"mut_rate\": 0.05, \"sigma\": 0.01},\n",
        "        {\"mut_rate\": 0.10, \"sigma\": 0.005},\n",
        "        {\"mut_rate\": 0.10, \"sigma\": 0.01},\n",
        "    ]\n",
        "\n",
        "    def run_once(cfg, eval_cap, time_s, seed):\n",
        "      pop_size = 24; elite_frac = 0.1; generations = 15\n",
        "      state = {\"evals\": 0, \"failures\": 0}\n",
        "      svec = safe_start_vec(model, k, start_vec=start_vec, noise_sigma=0.0, seed=seed)\n",
        "\n",
        "      raw = ga(model, k, val_loader,\n",
        "              start_vec=svec,\n",
        "              pop_size=pop_size, elite_frac=elite_frac,\n",
        "              mut_rate=cfg[\"mut_rate\"], sigma=cfg[\"sigma\"],\n",
        "              generations=generations, seed=seed)\n",
        "\n",
        "      vec = coerce_vec1d(raw)  # <<< ensure 1-D torch tensor\n",
        "      score = objective_from_vec_safe(model, k, vec, val_loader, state)  # higher = better\n",
        "      return {\"score\": float(score), \"evals\": int(state[\"evals\"])}\n",
        "\n",
        "\n",
        "    best = micro_tune(run_once, grid, hard_eval_cap=400, hard_time_s=3.0, seed=seed)\n",
        "    picked = best[\"picked_cfg\"]\n",
        "    # do a single slightly longer run with the winner (still small)\n",
        "    return picked, run_once(picked, eval_cap=600, time_s=5.0, seed=seed)\n"
      ],
      "metadata": {
        "id": "w-cs1ne9S4iE"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- RHC micro-tuning (tiny + simple) --------\n",
        "def rhc_quick_tune(model, k, val_loader, start_vec=None, seed=0):\n",
        "    grid = [\n",
        "        {\"sigma\": 0.01, \"restarts\": 0},\n",
        "        {\"sigma\": 0.01, \"restarts\": 2},\n",
        "        {\"sigma\": 0.05, \"restarts\": 0},\n",
        "        {\"sigma\": 0.05, \"restarts\": 2},\n",
        "    ]\n",
        "    def run_once(cfg, eval_cap, time_s, seed):\n",
        "        MICRO_BUDGET = 200\n",
        "        state = {\"evals\": 0, \"failures\": 0}\n",
        "        svec = safe_start_vec(model, k, start_vec=start_vec, noise_sigma=0.0, seed=seed)\n",
        "        raw = rhc(model, k, val_loader, start_vec=svec,\n",
        "                  sigma=cfg[\"sigma\"], restarts=cfg[\"restarts\"],\n",
        "                  budget=MICRO_BUDGET, seed=seed)\n",
        "        vec = coerce_vec1d(raw)\n",
        "        score = objective_from_vec_safe(model, k, vec, val_loader, state)\n",
        "        return {\"score\": float(score), \"evals\": int(state[\"evals\"])}\n",
        "    best = micro_tune(run_once, grid, hard_eval_cap=400, hard_time_s=3.0, seed=seed)\n",
        "    picked = best[\"picked_cfg\"]\n",
        "    return picked, run_once(picked, eval_cap=600, time_s=5.0, seed=seed)\n"
      ],
      "metadata": {
        "id": "bjckFN9Qq-PG"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- SA micro-tuning (matches your SA signature) ----------\n",
        "def sa_quick_tune(model, k, val_loader, start_vec=None, seed=0):\n",
        "    # tiny grid (4–6 configs max). Keep it small + fast.\n",
        "    grid = [\n",
        "        {\"T0\": 1.0, \"decay\": 0.995, \"sigma\": 0.01},\n",
        "        {\"T0\": 1.0, \"decay\": 0.990, \"sigma\": 0.01},\n",
        "        {\"T0\": 5.0, \"decay\": 0.995, \"sigma\": 0.01},\n",
        "        {\"T0\": 1.0, \"decay\": 0.995, \"sigma\": 0.05},\n",
        "    ]\n",
        "\n",
        "    def run_once(cfg, eval_cap, time_s, seed):\n",
        "        MICRO_BUDGET = 200  # very small\n",
        "        state = {\"evals\": 0, \"failures\": 0}\n",
        "        svec = safe_start_vec(model, k, start_vec=start_vec, noise_sigma=0.0, seed=seed)\n",
        "\n",
        "        raw = sa(model, k, val_loader,\n",
        "                 start_vec=svec,\n",
        "                 sigma=cfg[\"sigma\"],\n",
        "                 T0=cfg[\"T0\"],\n",
        "                 decay=cfg[\"decay\"],\n",
        "                 budget=MICRO_BUDGET,\n",
        "                 seed=seed)\n",
        "\n",
        "        vec = coerce_vec1d(raw)\n",
        "        score = objective_from_vec_safe(model, k, vec, val_loader, state)  # higher = better\n",
        "        return {\"score\": float(score), \"evals\": int(state[\"evals\"])}\n",
        "\n",
        "    best = micro_tune(run_once, grid, hard_eval_cap=400, hard_time_s=3.0, seed=seed)\n",
        "    picked = best[\"picked_cfg\"]\n",
        "    return picked, run_once(picked, eval_cap=600, time_s=5.0, seed=seed)\n"
      ],
      "metadata": {
        "id": "V1axlWPlq_Gz"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Coerce dict to vector\n",
        "import numpy as np, torch\n",
        "\n",
        "def coerce_vec1d(x):\n",
        "    \"\"\"Return a 1-D torch.float32 tensor from many possible RO outputs.\"\"\"\n",
        "    # If it's a dict, grab the most likely vector field\n",
        "    if isinstance(x, dict):\n",
        "        for key in (\"vec\", \"best_vec\", \"solution\", \"x\", \"theta\", \"w\"):\n",
        "            if key in x:\n",
        "                x = x[key]\n",
        "                break\n",
        "        else:\n",
        "            raise TypeError(\"RO result is a dict but no vector-like key was found.\")\n",
        "    # Now normalize to 1-D torch tensor\n",
        "    if isinstance(x, np.ndarray):\n",
        "        x = torch.from_numpy(x)\n",
        "    elif isinstance(x, (list, tuple)):\n",
        "        x = torch.tensor(x)\n",
        "    elif not torch.is_tensor(x):\n",
        "        raise TypeError(f\"Unsupported RO result type: {type(x)}\")\n",
        "    return x.reshape(-1).contiguous().to(torch.float32)\n"
      ],
      "metadata": {
        "id": "gQzOq9orsvKM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GA Run using MicroTuning\n",
        "k=1\n",
        "# Example for GA:\n",
        "picked_cfg, warm_result = ga_quick_tune(model, k, val_loader, start_vec=None, seed=0)\n",
        "print(\"GA picked:\", picked_cfg, \"warm score:\", warm_result[\"score\"])\n",
        "\n",
        "raw_final = ga(model, k, val_loader,\n",
        "               start_vec=safe_start_vec(model, k, seed=1),\n",
        "               pop_size=32, elite_frac=0.1,\n",
        "               mut_rate=picked_cfg[\"mut_rate\"], sigma=picked_cfg[\"sigma\"],\n",
        "               generations=40, seed=1)\n",
        "final_vec = coerce_vec1d(raw_final)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4kPjIeYzUc4",
        "outputId": "c2c424a1-c081-4fe1-e04a-70e12d16ce71"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GA picked: {'mut_rate': 0.05, 'sigma': 0.005} warm score: 0.6467700537882353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- SA final run (short; same style as your GA block) ----------\n",
        "picked_cfg, warm_res = sa_quick_tune(model, k, val_loader,\n",
        "                                     start_vec=safe_start_vec(model, k, seed=1),\n",
        "                                     seed=0)\n",
        "print(\"SA picked:\", picked_cfg, \"| warm score:\", warm_res[\"score\"])\n",
        "\n",
        "raw_final = sa(model, k, val_loader,\n",
        "               start_vec=safe_start_vec(model, k, seed=1),\n",
        "               sigma=picked_cfg[\"sigma\"],\n",
        "               T0=picked_cfg[\"T0\"],\n",
        "               decay=picked_cfg[\"decay\"],\n",
        "               budget=600,   # modest bump over micro\n",
        "               seed=1)\n",
        "\n",
        "final_vec = coerce_vec1d(raw_final)\n",
        "\n",
        "# Optional scoring for parity\n",
        "state = {\"evals\": 0, \"failures\": 0}\n",
        "final_score = objective_from_vec_safe(model, k, final_vec, val_loader, state)\n",
        "print(\"SA final score:\", float(final_score), \"| evals:\", int(state[\"evals\"]))\n"
      ],
      "metadata": {
        "id": "ib8RQpgupJk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0efb3883-df2c-4439-9a61-3b4ec4777f45"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SA picked: {'T0': 5.0, 'decay': 0.995, 'sigma': 0.01} | warm score: 0.6411196338502985\n",
            "SA final score: 0.6388245068098369 | evals: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- RHC final run (matches your GA/SA pattern) --------\n",
        "picked_cfg, warm_res = rhc_quick_tune(\n",
        "    model, k, val_loader, start_vec=safe_start_vec(model, k, seed=1), seed=0)\n",
        "print(\"RHC picked:\", picked_cfg, \"| warm score:\", warm_res[\"score\"])\n",
        "\n",
        "raw_final = rhc(model, k, val_loader,\n",
        "                start_vec=safe_start_vec(model, k, seed=1),\n",
        "                sigma=picked_cfg[\"sigma\"],\n",
        "                restarts=picked_cfg[\"restarts\"],\n",
        "                budget=600, seed=1)\n",
        "final_vec = coerce_vec1d(raw_final)\n",
        "\n",
        "state = {\"evals\": 0, \"failures\": 0}\n",
        "final_score = objective_from_vec_safe(model, k, final_vec, val_loader, state)\n",
        "print(\"RHC final score:\", float(final_score), \"| evals:\", int(state[\"evals\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8J8KhpmztRcd",
        "outputId": "46176dcf-9a22-45e9-c735-9e9a74172481"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RHC picked: {'sigma': 0.01, 'restarts': 0} | warm score: 0.6120349984419974\n",
            "RHC final score: 0.5693862814652292 | evals: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Freeze a single starting tail and reuse it for all algorithms\n",
        "start_vec = tail_to_vector(model, k).clone()\n",
        "\n",
        "def run_rhc_from_start():\n",
        "    return rhc(model, k, val_loader, start_vec=start_vec.clone(), sigma=0.01, restarts=0, budget=2000, seed=42)\n",
        "\n",
        "def run_sa_from_start():\n",
        "    return sa(model,  k, val_loader, start_vec=start_vec.clone(), sigma=0.01, T0=5.0, decay=0.995, budget=2000, seed=42)\n",
        "\n",
        "def run_ga_from_start():\n",
        "    pop, BUDGET = 32, 2000\n",
        "    gens = max(1, BUDGET // pop)\n",
        "    return ga(model, k, val_loader, start_vec=start_vec.clone(), pop_size=pop, elite_frac=0.125, mut_rate=0.05, sigma=0.005,\n",
        "              generations=gens, seed=42)\n",
        "\n",
        "# 2) After each run, restore the original tail so the next run starts clean\n",
        "orig_tail = start_vec.clone()\n",
        "rhc_res = run_rhc_from_start(); vector_to_tail(model, k, orig_tail)\n",
        "sa_res  = run_sa_from_start();  vector_to_tail(model, k, orig_tail)\n",
        "ga_res  = run_ga_from_start();  vector_to_tail(model, k, orig_tail)\n"
      ],
      "metadata": {
        "id": "esnSokggsLst"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# With Failure Rates, tyr to run this. Then, look into pickles.\n",
        "def plot_and_save_ro_progress(rhc_res, sa_res, ga_res,\n",
        "                              title=\"Part 1: RO progress (val loss vs evals)\",\n",
        "                              png_path=\"ro_progress.png\",\n",
        "                              pdf_path=\"ro_progress.pdf\"):\n",
        "    plt.figure(figsize=(3.4, 2.2))\n",
        "    for name, res in [(\"RHC\", rhc_res), (\"SA\", sa_res), (\"GA\", ga_res)]:\n",
        "        if \"history\" not in res or not res[\"history\"]:\n",
        "            continue\n",
        "        xs = [e for e, _ in res[\"history\"]]\n",
        "        ys = [b for _, b in res[\"history\"]]\n",
        "        # compute failure rate if available; else show \"n/a\"\n",
        "        if \"failures\" in res and \"evals\" in res and res[\"evals\"] > 0:\n",
        "            fr = res[\"failures\"] / max(1, res[\"evals\"])\n",
        "            label = f\"{name} (fail {fr:.1%})\"\n",
        "        else:\n",
        "            label = f\"{name} (fail n/a)\"\n",
        "        plt.plot(xs, ys, label=label)\n",
        "\n",
        "    plt.xlabel(\"Function evaluations (validation-loss calls)\")\n",
        "    plt.ylabel(\"Best-so-far validation loss\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(png_path, dpi=300, bbox_inches=\"tight\")\n",
        "    plt.savefig(pdf_path,          bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    print(f\"Saved: {png_path} and {pdf_path}\")\n",
        "\n",
        "plot_and_save_ro_progress(rhc_res, sa_res, ga_res,\n",
        "                          title=\"Hotel: RHC vs SA vs GA (val loss vs evals)\",\n",
        "                          png_path=\"ro_progress_hotel.png\",\n",
        "                          pdf_path=\"ro_progress_hotel.pdf\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "UqNyU56PKD0F",
        "outputId": "29247208-96f0-4900-b59d-6bceaa44222c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 340x220 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: ro_progress_hotel.png and ro_progress_hotel.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, matplotlib.pyplot as plt, pickle\n",
        "\n",
        "# ---- picked configs from your screenshot ----\n",
        "picked_rhc_cfg = {\"sigma\":0.01, \"restarts\":0, \"budget\":2000, \"seed\":42}\n",
        "picked_sa_cfg  = {\"sigma\":0.01, \"T0\":5.0, \"decay\":0.995, \"budget\":2000, \"seed\":42}\n",
        "picked_ga_cfg  = {\"pop_size\":32, \"elite_frac\":0.125, \"mut_rate\":0.05,\n",
        "                  \"sigma\":0.005, \"generations\":2000//32, \"seed\":42}\n",
        "\n",
        "with open(\"picked_cfgs.pkl\",\"wb\") as f:\n",
        "    pickle.dump({\"RHC\":picked_rhc_cfg,\"SA\":picked_sa_cfg,\"GA\":picked_ga_cfg}, f)\n",
        "\n",
        "# ---- multi-seed median±IQR plot, saved at 300 dpi ----\n",
        "N_SEEDS, SEEDS = 5, list(range(42, 42+5))\n",
        "S0 = tail_to_vector(model, k).clone()\n",
        "XS = np.arange(0, 2001, 10)\n",
        "\n",
        "run_rhc = lambda s: rhc(model,k,val_loader,start_vec=S0.clone(), **{**picked_rhc_cfg,\"seed\":s})\n",
        "run_sa  = lambda s: sa (model,k,val_loader,start_vec=S0.clone(), **{**picked_sa_cfg, \"seed\":s})\n",
        "run_ga  = lambda s: ga (model,k,val_loader,start_vec=S0.clone(), **{**picked_ga_cfg, \"seed\":s})\n",
        "\n",
        "def agg(run):\n",
        "    Ys, fr = [], []\n",
        "    for s in SEEDS:\n",
        "        res = run(s); h = np.array(res[\"history\"]); x,y = h[:,0],h[:,1]\n",
        "        Ys.append(np.interp(XS, x, y, left=y[0], right=y[-1]))\n",
        "        fr.append(res.get(\"failures\",0)/max(1,res.get(\"evals\",len(x))))\n",
        "    Y = np.vstack(Ys)\n",
        "    return np.median(Y,0), np.percentile(Y,25,0), np.percentile(Y,75,0), np.mean(fr)\n",
        "\n",
        "plt.figure(figsize=(3.8,2.4))\n",
        "for name, run in [(\"RHC\",run_rhc),(\"SA\",run_sa),(\"GA\",run_ga)]:\n",
        "    med,q1,q3,fr = agg(run)\n",
        "    plt.plot(XS, med, label=f\"{name} (fail {fr:.1%})\"); plt.fill_between(XS,q1,q3,alpha=0.2)\n",
        "plt.xlabel(\"Function evaluations\"); plt.ylabel(\"Best-so-far val loss\")\n",
        "plt.title(\"RO trajectories: median ± IQR over seeds\"); plt.grid(alpha=0.3); plt.legend()\n",
        "plt.tight_layout(); plt.savefig(\"ro_trajectories.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.savefig(\"ro_trajectories.pdf\", bbox_inches=\"tight\"); plt.show()\n",
        "print(\"Saved configs → picked_cfgs.pkl; plots → ro_trajectories.(png/pdf)\")\n"
      ],
      "metadata": {
        "id": "rESlnpgED-ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WEiL0fqlD_sk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}